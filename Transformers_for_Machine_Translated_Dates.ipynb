{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eff9df0-158c-48cf-8d30-4b3fa1611771",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'march 8 1758', '1758-03-08'\\n\",\n",
       " \"'17 february 1709', '1709-02-17'\\n\",\n",
       " \"'13 may 1786', '1786-05-13'\\n\",\n",
       " \"'17 june 1626', '1626-06-17'\\n\",\n",
       " \"'july 25 1851', '1851-07-25'\\n\",\n",
       " \"'1975 4 mar', '1975-03-04'\\n\",\n",
       " \"'16 march 1926', '1926-03-16'\\n\",\n",
       " \"'11 january 2043', '2043-01-11'\\n\",\n",
       " \"'sat 14 apr 2007', '2007-04-14'\\n\",\n",
       " \"'thu 1873 30 october', '1873-10-30'\\n\",\n",
       " \"'october 14 1584', '1584-10-14'\\n\",\n",
       " \"'may 4 1942', '1942-05-04'\\n\",\n",
       " \"'13/09/1650', '1650-09-13'\\n\",\n",
       " \"'24 july 1572', '1572-07-24'\\n\",\n",
       " \"'september 24 1946', '1946-09-24'\\n\",\n",
       " \"'jul 24 2030', '2030-07-24'\\n\",\n",
       " \"'tuesday 22 february 1853', '1853-02-22'\\n\",\n",
       " \"'november 24 1700', '1700-11-24'\\n\",\n",
       " \"'2 december 1633', '1633-12-02'\\n\",\n",
       " \"'tuesday 17 april 1798', '1798-04-17'\\n\",\n",
       " \"'september 10 1607', '1607-09-10'\\n\",\n",
       " \"'monday 1918 4 02', '1918-02-04'\\n\",\n",
       " \"'aug 29 1827', '1827-08-29'\\n\",\n",
       " \"'2014 13 mar', '2014-03-13'\\n\",\n",
       " \"'mar 13 1560', '1560-03-13'\\n\",\n",
       " \"'4 september 2046', '2046-09-04'\\n\",\n",
       " \"'november 28 1576', '1576-11-28'\\n\",\n",
       " \"'may 14 1735', '1735-05-14'\\n\",\n",
       " \"'june 17 1852', '1852-06-17'\\n\",\n",
       " \"'28 oct 2047', '2047-10-28'\\n\",\n",
       " \"'november 26 1803', '1803-11-26'\\n\",\n",
       " \"'saturday 11 may 2030', '2030-05-11'\\n\",\n",
       " \"'march 2 1937', '1937-03-02'\\n\",\n",
       " \"'aug 6 1898', '1898-08-06'\\n\",\n",
       " \"'tuesday may 28 2047', '2047-05-28'\\n\",\n",
       " \"'dec 31 1746', '1746-12-31'\\n\",\n",
       " \"'19 september 1687', '1687-09-19'\\n\",\n",
       " \"'14 april 1658', '1658-04-14'\\n\",\n",
       " \"'wednesday january 21 2065', '2065-01-21'\\n\",\n",
       " \"'may 3 1553', '1553-05-03'\\n\",\n",
       " \"'thursday january 22 1570', '1570-01-22'\\n\",\n",
       " \"'friday april 2 1869', '1869-04-02'\\n\",\n",
       " \"'june 19 1604', '1604-06-19'\\n\",\n",
       " \"'december 1 1701', '1701-12-01'\\n\",\n",
       " \"'2016 31 jul', '2016-07-31'\\n\",\n",
       " \"'apr 6 1924', '1924-04-06'\\n\",\n",
       " \"'thursday 1806 16 10', '1806-10-16'\\n\",\n",
       " \"'monday october 3 1763', '1763-10-03'\\n\",\n",
       " \"'august 12 1991', '1991-08-12'\\n\",\n",
       " \"'28 jul 1845', '1845-07-28'\\n\",\n",
       " \"'30 nov 1540', '1540-11-30'\\n\",\n",
       " \"'march 15 1978', '1978-03-15'\\n\",\n",
       " \"'august 19 1816', '1816-08-19'\\n\",\n",
       " \"'july 16 1737', '1737-07-16'\\n\",\n",
       " \"'monday december 18 1724', '1724-12-18'\\n\",\n",
       " \"'1970 17 july', '1970-07-17'\\n\",\n",
       " \"'friday 31 march 1690', '1690-03-31'\\n\",\n",
       " \"'06/03/1538', '1538-03-06'\\n\",\n",
       " \"'monday 2032 13 09', '2032-09-13'\\n\",\n",
       " \"'21/01/1535', '1535-01-21'\\n\",\n",
       " \"'8 october 1961', '1961-10-08'\\n\",\n",
       " \"'16 may 1677', '1677-05-16'\\n\",\n",
       " \"'24 nov 1578', '1578-11-24'\\n\",\n",
       " \"'may 24 1524', '1524-05-24'\\n\",\n",
       " \"'20 may 1914', '1914-05-20'\\n\",\n",
       " \"'february 12 1588', '1588-02-12'\\n\",\n",
       " \"'may 20 1631', '1631-05-20'\\n\",\n",
       " \"'october 9 1672', '1672-10-09'\\n\",\n",
       " \"'june 25 2015', '2015-06-25'\\n\",\n",
       " \"'jul 23 1999', '1999-07-23'\\n\",\n",
       " \"'1723 6 feb', '1723-02-06'\\n\",\n",
       " \"'nov 16 2037', '2037-11-16'\\n\",\n",
       " \"'nov 21 1522', '1522-11-21'\\n\",\n",
       " \"'may 21 1808', '1808-05-21'\\n\",\n",
       " \"'tuesday 1793 1 01', '1793-01-01'\\n\",\n",
       " \"'thursday 2053 4 12', '2053-12-04'\\n\",\n",
       " \"'mon 28 apr 2059', '2059-04-28'\\n\",\n",
       " \"'october 2 1551', '1551-10-02'\\n\",\n",
       " \"'july 25 1962', '1962-07-25'\\n\",\n",
       " \"'12 dec 1784', '1784-12-12'\\n\",\n",
       " \"'may 20 2027', '2027-05-20'\\n\",\n",
       " \"'aug 20 1975', '1975-08-20'\\n\",\n",
       " \"'wednesday march 31 1632', '1632-03-31'\\n\",\n",
       " \"'tuesday october 21 1997', '1997-10-21'\\n\",\n",
       " \"'22 november 2035', '2035-11-22'\\n\",\n",
       " \"'aug 6 1683', '1683-08-06'\\n\",\n",
       " \"'jun 10 1525', '1525-06-10'\\n\",\n",
       " \"'sunday 1790 26 09', '1790-09-26'\\n\",\n",
       " \"'1661 14 april', '1661-04-14'\\n\",\n",
       " \"'december 24 1716', '1716-12-24'\\n\",\n",
       " \"'18 july 1699', '1699-07-18'\\n\",\n",
       " \"'aug 20 1903', '1903-08-20'\\n\",\n",
       " \"'friday may 23 1851', '1851-05-23'\\n\",\n",
       " \"'12/03/1555', '1555-03-12'\\n\",\n",
       " \"'may 14 1697', '1697-05-14'\\n\",\n",
       " \"'12 january 1751', '1751-01-12'\\n\",\n",
       " \"'tuesday august 6 1799', '1799-08-06'\\n\",\n",
       " \"'saturday february 7 2071', '2071-02-07'\\n\",\n",
       " \"'jul 2 1860', '1860-07-02'\\n\",\n",
       " \"'1880 11 january', '1880-01-11'\\n\",\n",
       " \"'27 june 1569', '1569-06-27'\\n\",\n",
       " \"'november 21 2028', '2028-11-21'\\n\",\n",
       " \"'feb 8 2001', '2001-02-08'\\n\",\n",
       " \"'6 november 2015', '2015-11-06'\\n\",\n",
       " \"'1877 20 may', '1877-05-20'\\n\",\n",
       " \"'1665 18 jul', '1665-07-18'\\n\",\n",
       " \"'april 1 2057', '2057-04-01'\\n\",\n",
       " \"'april 14 1829', '1829-04-14'\\n\",\n",
       " \"'thu 23 jul 1542', '1542-07-23'\\n\",\n",
       " \"'sep 5 1894', '1894-09-05'\\n\",\n",
       " \"'jul 15 1529', '1529-07-15'\\n\",\n",
       " \"'april 6 1697', '1697-04-06'\\n\",\n",
       " \"'feb 17 1732', '1732-02-17'\\n\",\n",
       " \"'february 15 1751', '1751-02-15'\\n\",\n",
       " \"'2 april 1699', '1699-04-02'\\n\",\n",
       " \"'nov 1 1550', '1550-11-01'\\n\",\n",
       " \"'april 30 2015', '2015-04-30'\\n\",\n",
       " \"'sunday 2042 21 09', '2042-09-21'\\n\",\n",
       " \"'oct 20 1604', '1604-10-20'\\n\",\n",
       " \"'october 19 1562', '1562-10-19'\\n\",\n",
       " \"'wednesday february 23 1910', '1910-02-23'\\n\",\n",
       " \"'september 14 1587', '1587-09-14'\\n\",\n",
       " \"'apr 21 1580', '1580-04-21'\\n\",\n",
       " \"'31 aug 1774', '1774-08-31'\\n\",\n",
       " \"'sun 1617 5 march', '1617-03-05'\\n\",\n",
       " \"'13 may 2031', '2031-05-13'\\n\",\n",
       " \"'june 8 1954', '1954-06-08'\\n\",\n",
       " \"'29 july 1570', '1570-07-29'\\n\",\n",
       " \"'jul 15 1880', '1880-07-15'\\n\",\n",
       " \"'23 february 1862', '1862-02-23'\\n\",\n",
       " \"'28 august 1673', '1673-08-28'\\n\",\n",
       " \"'3 july 1925', '1925-07-03'\\n\",\n",
       " \"'december 14 1521', '1521-12-14'\\n\",\n",
       " \"'october 20 1825', '1825-10-20'\\n\",\n",
       " \"'october 30 1654', '1654-10-30'\\n\",\n",
       " \"'july 18 2026', '2026-07-18'\\n\",\n",
       " \"'14/11/1797', '1797-11-14'\\n\",\n",
       " \"'april 6 1821', '1821-04-06'\\n\",\n",
       " \"'wednesday march 18 1592', '1592-03-18'\\n\",\n",
       " \"'30 july 1804', '1804-07-30'\\n\",\n",
       " \"'jul 19 1653', '1653-07-19'\\n\",\n",
       " \"'tuesday december 4 1579', '1579-12-04'\\n\",\n",
       " \"'jun 10 1609', '1609-06-10'\\n\",\n",
       " \"'6 december 1619', '1619-12-06'\\n\",\n",
       " \"'9/4/64', '1764-09-04'\\n\",\n",
       " \"'may 2 1967', '1967-05-02'\\n\",\n",
       " \"'january 4 1946', '1946-01-04'\\n\",\n",
       " \"'march 28 1638', '1638-03-28'\\n\",\n",
       " \"'1688 27 october', '1688-10-27'\\n\",\n",
       " \"'23 may 1741', '1741-05-23'\\n\",\n",
       " \"'april 26 2036', '2036-04-26'\\n\",\n",
       " \"'23/05/2004', '2004-05-23'\\n\",\n",
       " \"'december 24 2050', '2050-12-24'\\n\",\n",
       " \"'november 10 1663', '1663-11-10'\\n\",\n",
       " \"'november 17 1570', '1570-11-17'\\n\",\n",
       " \"'1 april 2034', '2034-04-01'\\n\",\n",
       " \"'sat 17 feb 1996', '1996-02-17'\\n\",\n",
       " \"'aug 5 1710', '1710-08-05'\\n\",\n",
       " \"'sunday 1709 14 04', '1709-04-14'\\n\",\n",
       " \"'wednesday october 25 1916', '1916-10-25'\\n\",\n",
       " \"'17/04/1823', '1823-04-17'\\n\",\n",
       " \"'oct 26 2014', '2014-10-26'\\n\",\n",
       " \"'jun 18 1679', '1679-06-18'\\n\",\n",
       " \"'9/6/20', '2020-09-06'\\n\",\n",
       " \"'10 sep 1594', '1594-09-10'\\n\",\n",
       " \"'15 january 1793', '1793-01-15'\\n\",\n",
       " \"'april 22 1787', '1787-04-22'\\n\",\n",
       " \"'june 13 1741', '1741-06-13'\\n\",\n",
       " \"'april 22 2055', '2055-04-22'\\n\",\n",
       " \"'1534 7 june', '1534-06-07'\\n\",\n",
       " \"'jul 22 1637', '1637-07-22'\\n\",\n",
       " \"'3 september 1808', '1808-09-03'\\n\",\n",
       " \"'apr 25 1956', '1956-04-25'\\n\",\n",
       " \"'wed 8 sep 1694', '1694-09-08'\\n\",\n",
       " \"'mon 1613 31 december', '1612-12-31'\\n\",\n",
       " \"'5/25/49', '1649-05-25'\\n\",\n",
       " \"'sun 1525 3 january', '1526-01-03'\\n\",\n",
       " \"'1862 16 july', '1862-07-16'\\n\",\n",
       " \"'wednesday january 13 2016', '2016-01-13'\\n\",\n",
       " \"'fri 1743 22 march', '1743-03-22'\\n\",\n",
       " \"'june 3 1671', '1671-06-03'\\n\",\n",
       " \"'july 7 1932', '1932-07-07'\\n\",\n",
       " \"'13 may 1547', '1547-05-13'\\n\",\n",
       " \"'saturday 27 april 1996', '1996-04-27'\\n\",\n",
       " \"'friday may 8 1767', '1767-05-08'\\n\",\n",
       " \"'tuesday february 25 1536', '1536-02-25'\\n\",\n",
       " \"'october 20 2071', '2071-10-20'\\n\",\n",
       " \"'september 19 2024', '2024-09-19'\\n\",\n",
       " \"'september 20 1661', '1661-09-20'\\n\",\n",
       " \"'june 22 1574', '1574-06-22'\\n\",\n",
       " \"'4 jan 1867', '1867-01-04'\\n\",\n",
       " \"'3 december 1933', '1933-12-03'\\n\",\n",
       " \"'thursday 9 march 2006', '2006-03-09'\\n\",\n",
       " \"'20 august 1851', '1851-08-20'\\n\",\n",
       " \"'fri 5 feb 1745', '1745-02-05'\\n\",\n",
       " \"'1861 27 sep', '1861-09-27'\\n\",\n",
       " \"'sunday july 16 1950', '1950-07-16'\\n\",\n",
       " \"'sun 2024 27 october', '2024-10-27'\\n\",\n",
       " \"'august 9 1910', '1910-08-09'\\n\",\n",
       " \"'tue 13 nov 1584', '1584-11-13'\\n\",\n",
       " \"'02/11/1742', '1742-11-02'\\n\",\n",
       " \"'monday march 20 1876', '1876-03-20'\\n\",\n",
       " \"'april 8 1838', '1838-04-08'\\n\",\n",
       " \"'14 february 1656', '1656-02-14'\\n\",\n",
       " \"'7 june 1779', '1779-06-07'\\n\",\n",
       " \"'august 15 1567', '1567-08-15'\\n\",\n",
       " \"'march 24 1554', '1554-03-24'\\n\",\n",
       " \"'20 june 1776', '1776-06-20'\\n\",\n",
       " \"'24 september 1910', '1910-09-24'\\n\",\n",
       " \"'tuesday 1665 16 06', '1665-06-16'\\n\",\n",
       " \"'1987 27 august', '1987-08-27'\\n\",\n",
       " \"'mon 1530 8 december', '1530-12-08'\\n\",\n",
       " \"'19 december 1767', '1767-12-19'\\n\",\n",
       " \"'7/27/80', '1980-07-27'\\n\",\n",
       " \"'saturday december 29 1646', '1646-12-29'\\n\",\n",
       " \"'17 february 1923', '1923-02-17'\\n\",\n",
       " \"'17 january 2005', '2005-01-17'\\n\",\n",
       " \"'september 28 2030', '2030-09-28'\\n\",\n",
       " \"'27 march 1557', '1557-03-27'\\n\",\n",
       " \"'thursday april 13 1752', '1752-04-13'\\n\",\n",
       " \"'11/6/92', '1592-11-06'\\n\",\n",
       " \"'8 september 1753', '1753-09-08'\\n\",\n",
       " \"'friday 1538 19 08', '1538-08-19'\\n\",\n",
       " \"'16 february 1885', '1885-02-16'\\n\",\n",
       " \"'mar 25 1864', '1864-03-25'\\n\",\n",
       " \"'friday september 26 1580', '1580-09-26'\\n\",\n",
       " \"'1569 24 june', '1569-06-24'\\n\",\n",
       " \"'tuesday august 9 1803', '1803-08-09'\\n\",\n",
       " \"'may 20 1761', '1761-05-20'\\n\",\n",
       " \"'thursday july 29 1779', '1779-07-29'\\n\",\n",
       " \"'jun 29 1898', '1898-06-29'\\n\",\n",
       " \"'17 june 1919', '1919-06-17'\\n\",\n",
       " \"'oct 25 1908', '1908-10-25'\\n\",\n",
       " \"'aug 27 1588', '1588-08-27'\\n\",\n",
       " \"'december 15 1561', '1561-12-15'\\n\",\n",
       " \"'wed 1766 1 january', '1766-01-01'\\n\",\n",
       " \"'may 16 1672', '1672-05-16'\\n\",\n",
       " \"'29/10/1987', '1987-10-29'\\n\",\n",
       " \"'15 july 1830', '1830-07-15'\\n\",\n",
       " \"'6/20/38', '1738-06-20'\\n\",\n",
       " \"'sun 27 jun 1593', '1593-06-27'\\n\",\n",
       " \"'1648 6 may', '1648-05-06'\\n\",\n",
       " \"'17 january 1674', '1674-01-17'\\n\",\n",
       " \"'tue 17 may 1633', '1633-05-17'\\n\",\n",
       " \"'01/10/1790', '1790-10-01'\\n\",\n",
       " \"'29 june 1845', '1845-06-29'\\n\",\n",
       " \"'june 8 1915', '1915-06-08'\\n\",\n",
       " \"'saturday 1966 14 05', '1966-05-14'\\n\",\n",
       " \"'2/6/16', '1616-02-06'\\n\",\n",
       " \"'august 13 1831', '1831-08-13'\\n\",\n",
       " \"'august 18 1604', '1604-08-18'\\n\",\n",
       " \"'september 7 1943', '1943-09-07'\\n\",\n",
       " \"'fri 1645 19 may', '1645-05-19'\\n\",\n",
       " \"'saturday 6 february 1565', '1565-02-06'\\n\",\n",
       " \"'mon 1854 24 april', '1854-04-24'\\n\",\n",
       " \"'tue 1882 1 august', '1882-08-01'\\n\",\n",
       " \"'september 18 1869', '1869-09-18'\\n\",\n",
       " \"'mar 1 2055', '2055-03-01'\\n\",\n",
       " \"'december 27 1874', '1874-12-27'\\n\",\n",
       " \"'3/2/80', '1680-03-02'\\n\",\n",
       " \"'1665 23 oct', '1665-10-23'\\n\",\n",
       " \"'may 13 2014', '2014-05-13'\\n\",\n",
       " \"'june 21 1777', '1777-06-21'\\n\",\n",
       " \"'sunday february 28 1762', '1762-02-28'\\n\",\n",
       " \"'november 16 1809', '1809-11-16'\\n\",\n",
       " \"'28 feb 1860', '1860-02-28'\\n\",\n",
       " \"'february 2 2014', '2014-02-02'\\n\",\n",
       " \"'june 2 2018', '2018-06-02'\\n\",\n",
       " \"'wednesday 1625 12 02', '1625-02-12'\\n\",\n",
       " \"'march 4 1847', '1847-03-04'\\n\",\n",
       " \"'17 nov 1665', '1665-11-17'\\n\",\n",
       " \"'10/6/91', '1991-10-06'\\n\",\n",
       " \"'oct 14 1618', '1618-10-14'\\n\",\n",
       " \"'jan 30 1882', '1882-01-30'\\n\",\n",
       " \"'friday april 26 1697', '1697-04-26'\\n\",\n",
       " \"'tuesday 21 december 1920', '1920-12-21'\\n\",\n",
       " \"'9 apr 1947', '1947-04-09'\\n\",\n",
       " \"'sep 4 1842', '1842-09-04'\\n\",\n",
       " \"'november 23 2062', '2062-11-23'\\n\",\n",
       " \"'1830 24 nov', '1830-11-24'\\n\",\n",
       " \"'october 24 1829', '1829-10-24'\\n\",\n",
       " \"'sep 28 1626', '1626-09-28'\\n\",\n",
       " \"'sep 26 1697', '1697-09-26'\\n\",\n",
       " \"'27 june 1596', '1596-06-27'\\n\",\n",
       " \"'24 december 2049', '2049-12-24'\\n\",\n",
       " \"'22 october 1898', '1898-10-22'\\n\",\n",
       " \"'wednesday 1838 1 08', '1838-08-01'\\n\",\n",
       " \"'saturday april 1 1690', '1690-04-01'\\n\",\n",
       " \"'1585 29 september', '1585-09-29'\\n\",\n",
       " \"'tuesday november 26 1658', '1658-11-26'\\n\",\n",
       " \"'5/26/05', '1905-05-26'\\n\",\n",
       " \"'thu 26 may 1577', '1577-05-26'\\n\",\n",
       " \"'1720 3 july', '1720-07-03'\\n\",\n",
       " \"'14 september 1523', '1523-09-14'\\n\",\n",
       " \"'19/11/2021', '2021-11-19'\\n\",\n",
       " \"'17 september 1560', '1560-09-17'\\n\",\n",
       " \"'5 august 1711', '1711-08-05'\\n\",\n",
       " \"'jul 12 1945', '1945-07-12'\\n\",\n",
       " \"'feb 14 1801', '1801-02-14'\\n\",\n",
       " \"'20/09/1896', '1896-09-20'\\n\",\n",
       " \"'21 november 1822', '1822-11-21'\\n\",\n",
       " \"'31 march 1752', '1752-03-31'\\n\",\n",
       " \"'29 october 1811', '1811-10-29'\\n\",\n",
       " \"'22 february 1946', '1946-02-22'\\n\",\n",
       " \"'dec 2 1951', '1951-12-02'\\n\",\n",
       " \"'august 10 1699', '1699-08-10'\\n\",\n",
       " \"'1602 15 august', '1602-08-15'\\n\",\n",
       " \"'saturday october 14 1967', '1967-10-14'\\n\",\n",
       " \"'1641 18 mar', '1641-03-18'\\n\",\n",
       " \"'mon 12 nov 1849', '1849-11-12'\\n\",\n",
       " \"'22 november 1542', '1542-11-22'\\n\",\n",
       " \"'october 17 1800', '1800-10-17'\\n\",\n",
       " \"'thursday january 3 1669', '1669-01-03'\\n\",\n",
       " \"'december 30 1982', '1982-12-30'\\n\",\n",
       " \"'9 february 1767', '1767-02-09'\\n\",\n",
       " \"'wednesday may 11 1672', '1672-05-11'\\n\",\n",
       " \"'january 18 2039', '2039-01-18'\\n\",\n",
       " \"'august 3 1882', '1882-08-03'\\n\",\n",
       " \"'thursday 1789 9 07', '1789-07-09'\\n\",\n",
       " \"'2 october 1994', '1994-10-02'\\n\",\n",
       " \"'27 mar 1755', '1755-03-27'\\n\",\n",
       " \"'21 nov 2003', '2003-11-21'\\n\",\n",
       " \"'august 18 1823', '1823-08-18'\\n\",\n",
       " \"'mar 30 1570', '1570-03-30'\\n\",\n",
       " \"'monday 6 february 1651', '1651-02-06'\\n\",\n",
       " \"'sunday june 9 1546', '1546-06-09'\\n\",\n",
       " \"'january 31 1724', '1724-01-31'\\n\",\n",
       " \"'2001 23 mar', '2001-03-23'\\n\",\n",
       " \"'1630 9 aug', '1630-08-09'\\n\",\n",
       " \"'april 14 1633', '1633-04-14'\\n\",\n",
       " \"'7 may 2013', '2013-05-07'\\n\",\n",
       " \"'1 september 1660', '1660-09-01'\\n\",\n",
       " \"'june 20 1763', '1763-06-20'\\n\",\n",
       " \"'sep 18 1643', '1643-09-18'\\n\",\n",
       " \"'mar 9 1902', '1902-03-09'\\n\",\n",
       " \"'october 20 2054', '2054-10-20'\\n\",\n",
       " \"'april 25 1793', '1793-04-25'\\n\",\n",
       " \"'august 13 1530', '1530-08-13'\\n\",\n",
       " \"'november 19 1759', '1759-11-19'\\n\",\n",
       " \"'1936 2 jul', '1936-07-02'\\n\",\n",
       " \"'november 18 1769', '1769-11-18'\\n\",\n",
       " \"'15 october 1594', '1594-10-15'\\n\",\n",
       " \"'saturday august 26 1702', '1702-08-26'\\n\",\n",
       " \"'1960 21 november', '1960-11-21'\\n\",\n",
       " \"'august 3 1907', '1907-08-03'\\n\",\n",
       " \"'october 9 1554', '1554-10-09'\\n\",\n",
       " \"'feb 14 1961', '1961-02-14'\\n\",\n",
       " \"'saturday november 11 2028', '2028-11-11'\\n\",\n",
       " \"'wednesday june 6 1590', '1590-06-06'\\n\",\n",
       " \"'march 3 1678', '1678-03-03'\\n\",\n",
       " \"'march 19 1694', '1694-03-19'\\n\",\n",
       " \"'tuesday may 7 1737', '1737-05-07'\\n\",\n",
       " \"'12/29/44', '1844-12-29'\\n\",\n",
       " \"'wednesday 11 october 1673', '1673-10-11'\\n\",\n",
       " \"'wed 10 nov 1649', '1649-11-10'\\n\",\n",
       " \"'03/03/1702', '1702-03-03'\\n\",\n",
       " \"'2068 5 sep', '2068-09-05'\\n\",\n",
       " \"'january 25 1945', '1945-01-25'\\n\",\n",
       " \"'30 september 1524', '1524-09-30'\\n\",\n",
       " \"'1953 9 apr', '1953-04-09'\\n\",\n",
       " \"'06/05/1719', '1719-05-06'\\n\",\n",
       " \"'october 22 1992', '1992-10-22'\\n\",\n",
       " \"'1928 7 december', '1928-12-07'\\n\",\n",
       " \"'jun 9 1632', '1632-06-09'\\n\",\n",
       " \"'tuesday 2020 7 04', '2020-04-07'\\n\",\n",
       " \"'9 jul 1878', '1878-07-09'\\n\",\n",
       " \"'jan 15 1839', '1839-01-15'\\n\",\n",
       " \"'august 15 1673', '1673-08-15'\\n\",\n",
       " \"'19 april 1972', '1972-04-19'\\n\",\n",
       " \"'jan 18 2010', '2010-01-18'\\n\",\n",
       " \"'10/22/00', '1900-10-22'\\n\",\n",
       " \"'25 aug 2062', '2062-08-25'\\n\",\n",
       " \"'1578 13 march', '1578-03-13'\\n\",\n",
       " \"'mon 1972 31 july', '1972-07-31'\\n\",\n",
       " \"'23 mar 1714', '1714-03-23'\\n\",\n",
       " \"'wed 21 oct 2020', '2020-10-21'\\n\",\n",
       " \"'21 jul 2034', '2034-07-21'\\n\",\n",
       " \"'sat 1525 14 november', '1525-11-14'\\n\",\n",
       " \"'10/11/1830', '1830-11-10'\\n\",\n",
       " \"'march 14 2047', '2047-03-14'\\n\",\n",
       " \"'feb 17 1629', '1629-02-17'\\n\",\n",
       " \"'sun 1950 4 june', '1950-06-04'\\n\",\n",
       " \"'november 16 1599', '1599-11-16'\\n\",\n",
       " \"'march 12 1652', '1652-03-12'\\n\",\n",
       " \"'march 1 1830', '1830-03-01'\\n\",\n",
       " \"'thursday october 31 1557', '1557-10-31'\\n\",\n",
       " \"'11/5/57', '1957-11-05'\\n\",\n",
       " \"'tue 1 oct 1918', '1918-10-01'\\n\",\n",
       " \"'24 sep 1910', '1910-09-24'\\n\",\n",
       " \"'friday april 20 1832', '1832-04-20'\\n\",\n",
       " \"'friday 26 march 1965', '1965-03-26'\\n\",\n",
       " \"'sep 7 1877', '1877-09-07'\\n\",\n",
       " \"'8/30/32', '2032-08-30'\\n\",\n",
       " \"'27 december 1674', '1674-12-27'\\n\",\n",
       " \"'1528 29 aug', '1528-08-29'\\n\",\n",
       " \"'sunday august 24 1597', '1597-08-24'\\n\",\n",
       " \"'tuesday january 5 1790', '1790-01-05'\\n\",\n",
       " \"'10/16/36', '1736-10-16'\\n\",\n",
       " \"'3 june 1986', '1986-06-03'\\n\",\n",
       " \"'15 september 1584', '1584-09-15'\\n\",\n",
       " \"'1750 25 october', '1750-10-25'\\n\",\n",
       " \"'5/31/44', '1944-05-31'\\n\",\n",
       " \"'may 24 1589', '1589-05-24'\\n\",\n",
       " \"'thursday 1678 5 05', '1678-05-05'\\n\",\n",
       " \"'12/22/57', '1857-12-22'\\n\",\n",
       " \"'march 29 2004', '2004-03-29'\\n\",\n",
       " \"'october 2 2051', '2051-10-02'\\n\",\n",
       " \"'29 may 1724', '1724-05-29'\\n\",\n",
       " \"'may 19 1940', '1940-05-19'\\n\",\n",
       " \"'april 1 1617', '1617-04-01'\\n\",\n",
       " \"'saturday november 28 1987', '1987-11-28'\\n\",\n",
       " \"'march 7 1679', '1679-03-07'\\n\",\n",
       " \"'09/05/1870', '1870-05-09'\\n\",\n",
       " \"'january 28 1594', '1594-01-28'\\n\",\n",
       " \"'wednesday april 25 1849', '1849-04-25'\\n\",\n",
       " \"'8 october 1845', '1845-10-08'\\n\",\n",
       " \"'jun 24 1805', '1805-06-24'\\n\",\n",
       " \"'november 3 1680', '1680-11-03'\\n\",\n",
       " \"'jan 24 1752', '1752-01-24'\\n\",\n",
       " \"'october 29 1940', '1940-10-29'\\n\",\n",
       " \"'20 march 1912', '1912-03-20'\\n\",\n",
       " \"'monday august 3 1835', '1835-08-03'\\n\",\n",
       " \"'friday 1887 15 04', '1887-04-15'\\n\",\n",
       " \"'thursday december 26 1833', '1833-12-26'\\n\",\n",
       " \"'april 19 2050', '2050-04-19'\\n\",\n",
       " \"'aug 21 1685', '1685-08-21'\\n\",\n",
       " \"'12/07/1828', '1828-07-12'\\n\",\n",
       " \"'sunday december 2 1810', '1810-12-02'\\n\",\n",
       " \"'july 21 1526', '1526-07-21'\\n\",\n",
       " \"'nov 27 1632', '1632-11-27'\\n\",\n",
       " \"'wednesday july 10 1754', '1754-07-10'\\n\",\n",
       " \"'jan 6 1791', '1791-01-06'\\n\",\n",
       " \"'wed 1999 17 march', '1999-03-17'\\n\",\n",
       " \"'saturday 26 may 2029', '2029-05-26'\\n\",\n",
       " \"'thu 1955 10 march', '1955-03-10'\\n\",\n",
       " \"'11 march 1632', '1632-03-11'\\n\",\n",
       " \"'january 29 1891', '1891-01-29'\\n\",\n",
       " \"'june 23 1864', '1864-06-23'\\n\",\n",
       " \"'december 29 1925', '1925-12-29'\\n\",\n",
       " \"'february 4 1548', '1548-02-04'\\n\",\n",
       " \"'1918 5 march', '1918-03-05'\\n\",\n",
       " \"'october 4 1668', '1668-10-04'\\n\",\n",
       " \"'november 7 1853', '1853-11-07'\\n\",\n",
       " \"'september 16 1567', '1567-09-16'\\n\",\n",
       " \"'20 march 1732', '1732-03-20'\\n\",\n",
       " \"'may 10 1794', '1794-05-10'\\n\",\n",
       " \"'june 23 1856', '1856-06-23'\\n\",\n",
       " \"'23 february 1909', '1909-02-23'\\n\",\n",
       " \"'wednesday july 27 1549', '1549-07-27'\\n\",\n",
       " \"'16 sep 1911', '1911-09-16'\\n\",\n",
       " \"'august 16 2012', '2012-08-16'\\n\",\n",
       " \"'may 17 1539', '1539-05-17'\\n\",\n",
       " \"'thursday january 16 1659', '1659-01-16'\\n\",\n",
       " \"'25 may 2038', '2038-05-25'\\n\",\n",
       " \"'september 9 1688', '1688-09-09'\\n\",\n",
       " \"'9 may 1750', '1750-05-09'\\n\",\n",
       " \"'september 5 1836', '1836-09-05'\\n\",\n",
       " \"'29 january 1605', '1605-01-29'\\n\",\n",
       " \"'thursday 1587 1 10', '1587-10-01'\\n\",\n",
       " \"'june 24 1538', '1538-06-24'\\n\",\n",
       " \"'november 26 1565', '1565-11-26'\\n\",\n",
       " \"'sunday february 19 1860', '1860-02-19'\\n\",\n",
       " \"'17 july 1996', '1996-07-17'\\n\",\n",
       " \"'24 march 1950', '1950-03-24'\\n\",\n",
       " \"'may 27 2010', '2010-05-27'\\n\",\n",
       " \"'january 4 1672', '1672-01-04'\\n\",\n",
       " \"'1823 30 oct', '1823-10-30'\\n\",\n",
       " \"'wed 1760 7 may', '1760-05-07'\\n\",\n",
       " \"'feb 7 1807', '1807-02-07'\\n\",\n",
       " \"'1587 26 jan', '1587-01-26'\\n\",\n",
       " \"'tuesday 5 february 1546', '1546-02-05'\\n\",\n",
       " \"'jul 6 1771', '1771-07-06'\\n\",\n",
       " \"'30 apr 1978', '1978-04-30'\\n\",\n",
       " \"'12 july 2017', '2017-07-12'\\n\",\n",
       " \"'1622 22 apr', '1622-04-22'\\n\",\n",
       " \"'31 july 1899', '1899-07-31'\\n\",\n",
       " \"'aug 26 1850', '1850-08-26'\\n\",\n",
       " \"'16 january 1982', '1982-01-16'\\n\",\n",
       " \"'fri 1695 29 april', '1695-04-29'\\n\",\n",
       " \"'jan 3 1626', '1626-01-03'\\n\",\n",
       " \"'27 dec 1534', '1534-12-27'\\n\",\n",
       " \"'aug 14 1762', '1762-08-14'\\n\",\n",
       " \"'jan 4 1566', '1566-01-04'\\n\",\n",
       " \"'3 february 1852', '1852-02-03'\\n\",\n",
       " \"'3 november 1567', '1567-11-03'\\n\",\n",
       " \"'apr 9 1583', '1583-04-09'\\n\",\n",
       " \"'14 september 1576', '1576-09-14'\\n\",\n",
       " \"'7 sep 1865', '1865-09-07'\\n\",\n",
       " \"'june 7 1923', '1923-06-07'\\n\",\n",
       " \"'13 november 1724', '1724-11-13'\\n\",\n",
       " \"'jul 1 1634', '1634-07-01'\\n\",\n",
       " \"'19/11/1657', '1657-11-19'\\n\",\n",
       " \"'1880 15 june', '1880-06-15'\\n\",\n",
       " \"'saturday 1843 1 04', '1843-04-01'\\n\",\n",
       " \"'saturday 14 september 1743', '1743-09-14'\\n\",\n",
       " \"'march 14 1957', '1957-03-14'\\n\",\n",
       " \"'saturday may 6 1713', '1713-05-06'\\n\",\n",
       " \"'mon 25 apr 1768', '1768-04-25'\\n\",\n",
       " \"'sunday 1818 6 09', '1818-09-06'\\n\",\n",
       " \"'december 30 2066', '2066-12-30'\\n\",\n",
       " \"'19 november 1838', '1838-11-19'\\n\",\n",
       " \"'28 mar 1915', '1915-03-28'\\n\",\n",
       " \"'8 august 1803', '1803-08-08'\\n\",\n",
       " \"'sun 1741 7 may', '1741-05-07'\\n\",\n",
       " \"'sat 1680 14 september', '1680-09-14'\\n\",\n",
       " \"'aug 22 1959', '1959-08-22'\\n\",\n",
       " \"'tuesday august 4 1857', '1857-08-04'\\n\",\n",
       " \"'feb 17 2001', '2001-02-17'\\n\",\n",
       " \"'22 january 1885', '1885-01-22'\\n\",\n",
       " \"'aug 11 1978', '1978-08-11'\\n\",\n",
       " \"'apr 27 2066', '2066-04-27'\\n\",\n",
       " \"'thu 4 apr 1816', '1816-04-04'\\n\",\n",
       " \"'friday september 10 1751', '1751-09-10'\\n\",\n",
       " \"'june 30 1689', '1689-06-30'\\n\",\n",
       " \"'thursday 1750 22 01', '1750-01-22'\\n\",\n",
       " \"'1594 10 august', '1594-08-10'\\n\",\n",
       " \"'july 31 1860', '1860-07-31'\\n\",\n",
       " \"'friday october 5 1601', '1601-10-05'\\n\",\n",
       " \"'november 28 1926', '1926-11-28'\\n\",\n",
       " \"'april 25 1664', '1664-04-25'\\n\",\n",
       " \"'24 december 1747', '1747-12-24'\\n\",\n",
       " \"'dec 21 1867', '1867-12-21'\\n\",\n",
       " \"'wednesday october 2 2030', '2030-10-02'\\n\",\n",
       " \"'march 3 1948', '1948-03-03'\\n\",\n",
       " \"'12/8/61', '2061-12-08'\\n\",\n",
       " \"'22 november 1793', '1793-11-22'\\n\",\n",
       " \"'january 21 2008', '2008-01-21'\\n\",\n",
       " \"'3/28/39', '2039-03-28'\\n\",\n",
       " \"'monday october 23 2062', '2062-10-23'\\n\",\n",
       " \"'1727 11 sep', '1727-09-11'\\n\",\n",
       " \"'1848 24 february', '1848-02-24'\\n\",\n",
       " \"'13 november 1950', '1950-11-13'\\n\",\n",
       " \"'june 6 1533', '1533-06-06'\\n\",\n",
       " \"'may 17 1752', '1752-05-17'\\n\",\n",
       " \"'august 2 1536', '1536-08-02'\\n\",\n",
       " \"'27/02/1932', '1932-02-27'\\n\",\n",
       " \"'sunday january 31 1537', '1537-01-31'\\n\",\n",
       " \"'22 may 1605', '1605-05-22'\\n\",\n",
       " \"'may 9 1887', '1887-05-09'\\n\",\n",
       " \"'may 25 1796', '1796-05-25'\\n\",\n",
       " \"'nov 18 1792', '1792-11-18'\\n\",\n",
       " \"'sunday april 1 1742', '1742-04-01'\\n\",\n",
       " \"'sunday september 26 1655', '1655-09-26'\\n\",\n",
       " \"'december 4 1548', '1548-12-04'\\n\",\n",
       " \"'january 21 1701', '1701-01-21'\\n\",\n",
       " \"'saturday 10 march 1934', '1934-03-10'\\n\",\n",
       " \"'1706 30 december', '1706-12-30'\\n\",\n",
       " \"'1911 27 september', '1911-09-27'\\n\",\n",
       " \"'20 november 1555', '1555-11-20'\\n\",\n",
       " \"'december 2 1586', '1586-12-02'\\n\",\n",
       " \"'sunday 22 january 1911', '1911-01-22'\\n\",\n",
       " \"'february 14 1920', '1920-02-14'\\n\",\n",
       " \"'sunday 1742 7 10', '1742-10-07'\\n\",\n",
       " \"'january 7 2046', '2046-01-07'\\n\",\n",
       " \"'december 20 1626', '1626-12-20'\\n\",\n",
       " \"'16 jul 2033', '2033-07-16'\\n\",\n",
       " \"'saturday july 29 1797', '1797-07-29'\\n\",\n",
       " \"'sat 25 may 1624', '1624-05-25'\\n\",\n",
       " \"'october 15 1690', '1690-10-15'\\n\",\n",
       " \"'monday may 17 1576', '1576-05-17'\\n\",\n",
       " \"'wed 27 oct 1819', '1819-10-27'\\n\",\n",
       " \"'20 january 1788', '1788-01-20'\\n\",\n",
       " \"'29 june 1805', '1805-06-29'\\n\",\n",
       " \"'dec 25 1781', '1781-12-25'\\n\",\n",
       " \"'thursday july 21 1555', '1555-07-21'\\n\",\n",
       " \"'oct 23 1647', '1647-10-23'\\n\",\n",
       " \"'15 july 1884', '1884-07-15'\\n\",\n",
       " \"'28 april 1632', '1632-04-28'\\n\",\n",
       " \"'23/08/1931', '1931-08-23'\\n\",\n",
       " \"'thu 1653 16 january', '1653-01-16'\\n\",\n",
       " \"'dec 21 1978', '1978-12-21'\\n\",\n",
       " \"'5/2/47', '1947-05-02'\\n\",\n",
       " \"'wednesday 1827 10 01', '1827-01-10'\\n\",\n",
       " \"'september 25 1971', '1971-09-25'\\n\",\n",
       " \"'august 30 1570', '1570-08-30'\\n\",\n",
       " \"'10/1/54', '2054-10-01'\\n\",\n",
       " \"'friday 21 april 1561', '1561-04-21'\\n\",\n",
       " \"'30 june 1635', '1635-06-30'\\n\",\n",
       " \"'october 29 1949', '1949-10-29'\\n\",\n",
       " \"'april 18 1546', '1546-04-18'\\n\",\n",
       " \"'monday february 7 1701', '1701-02-07'\\n\",\n",
       " \"'may 22 2057', '2057-05-22'\\n\",\n",
       " \"'thursday 21 march 1811', '1811-03-21'\\n\",\n",
       " \"'sunday 29 august 1565', '1565-08-29'\\n\",\n",
       " \"'thu 2014 30 january', '2014-01-30'\\n\",\n",
       " \"'nov 18 1991', '1991-11-18'\\n\",\n",
       " \"'may 17 1594', '1594-05-17'\\n\",\n",
       " \"'wed 1637 28 october', '1637-10-28'\\n\",\n",
       " \"'15 sep 1793', '1793-09-15'\\n\",\n",
       " \"'june 24 1720', '1720-06-24'\\n\",\n",
       " \"'23 jun 1805', '1805-06-23'\\n\",\n",
       " \"'sun 3 jul 1859', '1859-07-03'\\n\",\n",
       " \"'friday july 4 1738', '1738-07-04'\\n\",\n",
       " \"'nov 3 1661', '1661-11-03'\\n\",\n",
       " \"'august 10 1606', '1606-08-10'\\n\",\n",
       " \"'friday 25 september 1959', '1959-09-25'\\n\",\n",
       " \"'2 march 1974', '1974-03-02'\\n\",\n",
       " \"'wednesday march 19 1636', '1636-03-19'\\n\",\n",
       " \"'sat 2029 23 june', '2029-06-23'\\n\",\n",
       " \"'8 june 1951', '1951-06-08'\\n\",\n",
       " \"'monday june 23 1884', '1884-06-23'\\n\",\n",
       " \"'12 november 2035', '2035-11-12'\\n\",\n",
       " \"'1898 27 mar', '1898-03-27'\\n\",\n",
       " \"'friday 17 may 2024', '2024-05-17'\\n\",\n",
       " \"'mon 18 jun 1618', '1618-06-18'\\n\",\n",
       " \"'june 23 1616', '1616-06-23'\\n\",\n",
       " \"'thursday january 13 2033', '2033-01-13'\\n\",\n",
       " \"'7 february 1711', '1711-02-07'\\n\",\n",
       " \"'13 november 1633', '1633-11-13'\\n\",\n",
       " \"'25 april 1610', '1610-04-25'\\n\",\n",
       " \"'21 may 1595', '1595-05-21'\\n\",\n",
       " \"'aug 27 1867', '1867-08-27'\\n\",\n",
       " \"'february 10 1740', '1740-02-10'\\n\",\n",
       " \"'15 may 1547', '1547-05-15'\\n\",\n",
       " \"'1/25/85', '1785-01-25'\\n\",\n",
       " \"'1/31/15', '1715-01-31'\\n\",\n",
       " \"'friday 1944 29 09', '1944-09-29'\\n\",\n",
       " \"'wednesday november 13 1568', '1568-11-13'\\n\",\n",
       " \"'jun 12 1782', '1782-06-12'\\n\",\n",
       " \"'06/02/1845', '1845-02-06'\\n\",\n",
       " \"'may 18 1826', '1826-05-18'\\n\",\n",
       " \"'apr 8 2015', '2015-04-08'\\n\",\n",
       " \"'thursday june 20 1985', '1985-06-20'\\n\",\n",
       " \"'dec 28 1761', '1761-12-28'\\n\",\n",
       " \"'jun 28 1744', '1744-06-28'\\n\",\n",
       " \"'friday june 20 2070', '2070-06-20'\\n\",\n",
       " \"'december 3 2052', '2052-12-03'\\n\",\n",
       " \"'10 august 1814', '1814-08-10'\\n\",\n",
       " \"'march 9 1863', '1863-03-09'\\n\",\n",
       " \"'9 october 1545', '1545-10-09'\\n\",\n",
       " \"'21 february 1654', '1654-02-21'\\n\",\n",
       " \"'march 10 2054', '2054-03-10'\\n\",\n",
       " \"'dec 29 1588', '1588-12-29'\\n\",\n",
       " \"'sep 26 1573', '1573-09-26'\\n\",\n",
       " \"'mon 1815 31 july', '1815-07-31'\\n\",\n",
       " \"'december 10 1893', '1893-12-10'\\n\",\n",
       " \"'december 20 1738', '1738-12-20'\\n\",\n",
       " \"'23 november 1922', '1922-11-23'\\n\",\n",
       " \"'may 28 1903', '1903-05-28'\\n\",\n",
       " \"'12 october 2048', '2048-10-12'\\n\",\n",
       " \"'21 may 1703', '1703-05-21'\\n\",\n",
       " \"'dec 29 2024', '2024-12-29'\\n\",\n",
       " \"'friday 21 january 2050', '2050-01-21'\\n\",\n",
       " \"'25 december 1681', '1681-12-25'\\n\",\n",
       " \"'28/05/1601', '1601-05-28'\\n\",\n",
       " \"'tuesday march 12 1793', '1793-03-12'\\n\",\n",
       " \"'dec 1 2069', '2069-12-01'\\n\",\n",
       " \"'17 july 1600', '1600-07-17'\\n\",\n",
       " \"'july 1 2032', '2032-07-01'\\n\",\n",
       " \"'sunday 2024 10 03', '2024-03-10'\\n\",\n",
       " \"'8 july 2026', '2026-07-08'\\n\",\n",
       " \"'fri 1738 30 may', '1738-05-30'\\n\",\n",
       " \"'29/07/1737', '1737-07-29'\\n\",\n",
       " \"'june 8 1907', '1907-06-08'\\n\",\n",
       " \"'july 13 2066', '2066-07-13'\\n\",\n",
       " \"'friday december 7 1951', '1951-12-07'\\n\",\n",
       " \"'november 30 1711', '1711-11-30'\\n\",\n",
       " \"'feb 11 1878', '1878-02-11'\\n\",\n",
       " \"'7 oct 1922', '1922-10-07'\\n\",\n",
       " \"'2053 12 november', '2053-11-12'\\n\",\n",
       " \"'may 13 1628', '1628-05-13'\\n\",\n",
       " \"'sun 1781 27 may', '1781-05-27'\\n\",\n",
       " \"'may 26 1707', '1707-05-26'\\n\",\n",
       " \"'june 19 2034', '2034-06-19'\\n\",\n",
       " \"'fri 1789 6 february', '1789-02-06'\\n\",\n",
       " \"'28 september 1540', '1540-09-28'\\n\",\n",
       " \"'thursday august 14 2070', '2070-08-14'\\n\",\n",
       " \"'12/9/67', '1567-12-09'\\n\",\n",
       " \"'13 mar 1657', '1657-03-13'\\n\",\n",
       " \"'20 october 1962', '1962-10-20'\\n\",\n",
       " \"'august 7 1713', '1713-08-07'\\n\",\n",
       " \"'monday 13 march 1752', '1752-03-13'\\n\",\n",
       " \"'january 8 1702', '1702-01-08'\\n\",\n",
       " \"'thursday 30 october 1597', '1597-10-30'\\n\",\n",
       " \"'5/3/48', '1848-05-03'\\n\",\n",
       " \"'thursday 16 december 1971', '1971-12-16'\\n\",\n",
       " \"'2032 10 october', '2032-10-10'\\n\",\n",
       " \"'8/12/97', '1997-08-12'\\n\",\n",
       " \"'thu 1845 17 april', '1845-04-17'\\n\",\n",
       " \"'3/31/18', '1818-03-31'\\n\",\n",
       " \"'may 5 1698', '1698-05-05'\\n\",\n",
       " \"'thursday june 18 1750', '1750-06-18'\\n\",\n",
       " \"'4 january 1956', '1956-01-04'\\n\",\n",
       " \"'september 1 1653', '1653-09-01'\\n\",\n",
       " \"'wed 1882 18 january', '1882-01-18'\\n\",\n",
       " \"'fri 23 may 2031', '2031-05-23'\\n\",\n",
       " \"'thu 16 sep 1830', '1830-09-16'\\n\",\n",
       " \"'29 december 1881', '1881-12-29'\\n\",\n",
       " \"'january 29 1551', '1551-01-29'\\n\",\n",
       " \"'december 4 1694', '1694-12-04'\\n\",\n",
       " \"'8/6/50', '1550-08-06'\\n\",\n",
       " \"'1814 6 oct', '1814-10-06'\\n\",\n",
       " \"'oct 20 1802', '1802-10-20'\\n\",\n",
       " \"'19 february 1972', '1972-02-19'\\n\",\n",
       " \"'sat 1660 17 april', '1660-04-17'\\n\",\n",
       " \"'9 june 1611', '1611-06-09'\\n\",\n",
       " \"'24 november 1764', '1764-11-24'\\n\",\n",
       " \"'tuesday january 20 1846', '1846-01-20'\\n\",\n",
       " \"'november 9 1792', '1792-11-09'\\n\",\n",
       " \"'july 22 1826', '1826-07-22'\\n\",\n",
       " \"'friday october 31 1575', '1575-10-31'\\n\",\n",
       " \"'may 8 1527', '1527-05-08'\\n\",\n",
       " \"'may 8 1837', '1837-05-08'\\n\",\n",
       " \"'sunday 10 august 2025', '2025-08-10'\\n\",\n",
       " \"'9 july 1765', '1765-07-09'\\n\",\n",
       " \"'sat 28 jan 1556', '1556-01-28'\\n\",\n",
       " \"'fri 1604 9 april', '1604-04-09'\\n\",\n",
       " \"'mon 1570 7 december', '1570-12-07'\\n\",\n",
       " \"'sunday june 24 1838', '1838-06-24'\\n\",\n",
       " \"'1854 20 december', '1854-12-20'\\n\",\n",
       " \"'1580 20 march', '1580-03-20'\\n\",\n",
       " \"'06/08/2056', '2056-08-06'\\n\",\n",
       " \"'thu 1593 18 february', '1593-02-18'\\n\",\n",
       " \"'apr 12 1551', '1551-04-12'\\n\",\n",
       " \"'1763 27 dec', '1763-12-27'\\n\",\n",
       " \"'january 22 1789', '1789-01-22'\\n\",\n",
       " \"'17 oct 1568', '1568-10-17'\\n\",\n",
       " \"'december 18 1906', '1906-12-18'\\n\",\n",
       " \"'march 1 2071', '2071-03-01'\\n\",\n",
       " \"'9/8/79', '1779-09-08'\\n\",\n",
       " \"'saturday december 14 1686', '1686-12-14'\\n\",\n",
       " \"'saturday june 11 2005', '2005-06-11'\\n\",\n",
       " \"'monday 1815 6 11', '1815-11-06'\\n\",\n",
       " \"'april 25 1793', '1793-04-25'\\n\",\n",
       " \"'1993 3 mar', '1993-03-03'\\n\",\n",
       " \"'september 27 1908', '1908-09-27'\\n\",\n",
       " \"'monday 1838 29 01', '1838-01-29'\\n\",\n",
       " \"'monday august 2 2060', '2060-08-02'\\n\",\n",
       " \"'15 april 1587', '1587-04-15'\\n\",\n",
       " \"'mon 1861 11 november', '1861-11-11'\\n\",\n",
       " \"'november 26 1917', '1917-11-26'\\n\",\n",
       " \"'november 6 1778', '1778-11-06'\\n\",\n",
       " \"'1 november 1754', '1754-11-01'\\n\",\n",
       " \"'11 july 1792', '1792-07-11'\\n\",\n",
       " \"'june 2 1999', '1999-06-02'\\n\",\n",
       " \"'15 december 1553', '1553-12-15'\\n\",\n",
       " \"'july 23 1986', '1986-07-23'\\n\",\n",
       " \"'friday february 21 1710', '1710-02-21'\\n\",\n",
       " \"'19 august 1637', '1637-08-19'\\n\",\n",
       " \"'september 17 1941', '1941-09-17'\\n\",\n",
       " \"'sun 21 sep 1884', '1884-09-21'\\n\",\n",
       " \"'sun 1882 19 november', '1882-11-19'\\n\",\n",
       " \"'1693 24 dec', '1693-12-24'\\n\",\n",
       " \"'saturday 1631 11 10', '1631-10-11'\\n\",\n",
       " \"'7 march 1907', '1907-03-07'\\n\",\n",
       " \"'6 february 1648', '1648-02-06'\\n\",\n",
       " \"'october 4 2017', '2017-10-04'\\n\",\n",
       " \"'aug 17 1587', '1587-08-17'\\n\",\n",
       " \"'1675 23 may', '1675-05-23'\\n\",\n",
       " \"'march 25 1779', '1779-03-25'\\n\",\n",
       " \"'21/06/1700', '1700-06-21'\\n\",\n",
       " \"'december 26 1741', '1741-12-26'\\n\",\n",
       " \"'sunday february 4 1798', '1798-02-04'\\n\",\n",
       " \"'19 may 1971', '1971-05-19'\\n\",\n",
       " \"'november 13 1899', '1899-11-13'\\n\",\n",
       " \"'7 january 1835', '1835-01-07'\\n\",\n",
       " \"'monday 19 june 1747', '1747-06-19'\\n\",\n",
       " \"'friday 1569 24 01', '1569-01-24'\\n\",\n",
       " \"'november 2 1918', '1918-11-02'\\n\",\n",
       " \"'9 december 2011', '2011-12-09'\\n\",\n",
       " \"'31 january 1778', '1778-01-31'\\n\",\n",
       " \"'thursday 1765 15 08', '1765-08-15'\\n\",\n",
       " \"'21 november 1590', '1590-11-21'\\n\",\n",
       " \"'1631 25 apr', '1631-04-25'\\n\",\n",
       " \"'friday march 8 1793', '1793-03-08'\\n\",\n",
       " \"'june 12 1594', '1594-06-12'\\n\",\n",
       " \"'29/09/1824', '1824-09-29'\\n\",\n",
       " \"'friday july 9 1948', '1948-07-09'\\n\",\n",
       " \"'saturday december 11 1971', '1971-12-11'\\n\",\n",
       " \"'8 july 1637', '1637-07-08'\\n\",\n",
       " \"'16 february 1750', '1750-02-16'\\n\",\n",
       " \"'march 7 2038', '2038-03-07'\\n\",\n",
       " \"'friday april 5 2052', '2052-04-05'\\n\",\n",
       " \"'sun 6 sep 1812', '1812-09-06'\\n\",\n",
       " \"'22 june 1562', '1562-06-22'\\n\",\n",
       " \"'thu 13 jun 1867', '1867-06-13'\\n\",\n",
       " \"'april 14 1738', '1738-04-14'\\n\",\n",
       " \"'november 14 1950', '1950-11-14'\\n\",\n",
       " \"'20 november 1915', '1915-11-20'\\n\",\n",
       " \"'3 december 2042', '2042-12-03'\\n\",\n",
       " \"'friday february 4 1859', '1859-02-04'\\n\",\n",
       " \"'monday december 2 2041', '2041-12-02'\\n\",\n",
       " \"'21/02/1627', '1627-02-21'\\n\",\n",
       " \"'1647 15 may', '1647-05-15'\\n\",\n",
       " \"'mon 23 jul 1928', '1928-07-23'\\n\",\n",
       " \"'aug 11 1859', '1859-08-11'\\n\",\n",
       " \"'friday 24 february 1645', '1645-02-24'\\n\",\n",
       " \"'29 june 1918', '1918-06-29'\\n\",\n",
       " \"'14/06/1531', '1531-06-14'\\n\",\n",
       " \"'23 feb 1608', '1608-02-23'\\n\",\n",
       " \"'august 17 1794', '1794-08-17'\\n\",\n",
       " \"'june 6 1621', '1621-06-06'\\n\",\n",
       " \"'july 4 2027', '2027-07-04'\\n\",\n",
       " \"'wednesday april 27 1955', '1955-04-27'\\n\",\n",
       " \"'wednesday 1525 30 09', '1525-09-30'\\n\",\n",
       " \"'22 march 1828', '1828-03-22'\\n\",\n",
       " \"'1841 26 march', '1841-03-26'\\n\",\n",
       " \"'october 20 1938', '1938-10-20'\\n\",\n",
       " \"'19 aug 1542', '1542-08-19'\\n\",\n",
       " \"'1 december 1874', '1874-12-01'\\n\",\n",
       " \"'sunday april 16 1911', '1911-04-16'\\n\",\n",
       " \"'july 26 1959', '1959-07-26'\\n\",\n",
       " \"'3/1/73', '1573-03-01'\\n\",\n",
       " \"'thursday march 5 1992', '1992-03-05'\\n\",\n",
       " \"'sun 14 jan 1551', '1551-01-14'\\n\",\n",
       " \"'2052 7 august', '2052-08-07'\\n\",\n",
       " \"'2 december 1998', '1998-12-02'\\n\",\n",
       " \"'1861 27 march', '1861-03-27'\\n\",\n",
       " \"'october 13 1527', '1527-10-13'\\n\",\n",
       " \"'july 19 1893', '1893-07-19'\\n\",\n",
       " \"'january 30 1626', '1626-01-30'\\n\",\n",
       " \"'friday 17 september 1841', '1841-09-17'\\n\",\n",
       " \"'jun 8 1914', '1914-06-08'\\n\",\n",
       " \"'1 may 1830', '1830-05-01'\\n\",\n",
       " \"'29 september 2066', '2066-09-29'\\n\",\n",
       " \"'wed 21 nov 1781', '1781-11-21'\\n\",\n",
       " \"'thursday february 15 1551', '1551-02-15'\\n\",\n",
       " \"'1893 18 july', '1893-07-18'\\n\",\n",
       " \"'1791 6 may', '1791-05-06'\\n\",\n",
       " \"'january 19 1895', '1895-01-19'\\n\",\n",
       " \"'mar 7 1557', '1557-03-07'\\n\",\n",
       " \"'13 december 1705', '1705-12-13'\\n\",\n",
       " \"'september 26 1932', '1932-09-26'\\n\",\n",
       " \"'7 january 1722', '1722-01-07'\\n\",\n",
       " \"'december 2 1712', '1712-12-02'\\n\",\n",
       " \"'wed 1714 7 november', '1714-11-07'\\n\",\n",
       " \"'tuesday march 31 1772', '1772-03-31'\\n\",\n",
       " \"'26/06/1790', '1790-06-26'\\n\",\n",
       " \"'november 16 1896', '1896-11-16'\\n\",\n",
       " \"'wednesday 1735 16 11', '1735-11-16'\\n\",\n",
       " \"'13 february 1895', '1895-02-13'\\n\",\n",
       " \"'8/21/05', '1605-08-21'\\n\",\n",
       " \"'11/12/73', '1573-11-12'\\n\",\n",
       " \"'november 25 1580', '1580-11-25'\\n\",\n",
       " \"'1982 13 august', '1982-08-13'\\n\",\n",
       " \"'june 6 1605', '1605-06-06'\\n\",\n",
       " \"'sunday 26 may 1901', '1901-05-26'\\n\",\n",
       " \"'10 aug 1650', '1650-08-10'\\n\",\n",
       " \"'jun 8 1785', '1785-06-08'\\n\",\n",
       " \"'thu 10 sep 1846', '1846-09-10'\\n\",\n",
       " \"'tuesday january 3 1662', '1662-01-03'\\n\",\n",
       " \"'tuesday may 15 1888', '1888-05-15'\\n\",\n",
       " \"'thursday 28 may 1744', '1744-05-28'\\n\",\n",
       " \"'1724 5 jun', '1724-06-05'\\n\",\n",
       " \"'17 march 1902', '1902-03-17'\\n\",\n",
       " \"'1 december 1601', '1601-12-01'\\n\",\n",
       " \"'30 september 1807', '1807-09-30'\\n\",\n",
       " \"'5/18/94', '1894-05-18'\\n\",\n",
       " \"'26 january 1529', '1529-01-26'\\n\",\n",
       " \"'may 29 1812', '1812-05-29'\\n\",\n",
       " \"'may 17 1895', '1895-05-17'\\n\",\n",
       " \"'september 12 1657', '1657-09-12'\\n\",\n",
       " \"'saturday january 25 2014', '2014-01-25'\\n\",\n",
       " \"'thursday august 31 1606', '1606-08-31'\\n\",\n",
       " \"'april 7 1700', '1700-04-07'\\n\",\n",
       " \"'thursday 1637 9 04', '1637-04-09'\\n\",\n",
       " \"'jun 6 1913', '1913-06-06'\\n\",\n",
       " \"'24/10/1595', '1595-10-24'\\n\",\n",
       " \"'2/3/26', '1826-02-03'\\n\",\n",
       " \"'may 6 1735', '1735-05-06'\\n\",\n",
       " \"'tuesday june 27 1865', '1865-06-27'\\n\",\n",
       " \"'1642 25 mar', '1642-03-25'\\n\",\n",
       " \"'tuesday 1552 24 06', '1552-06-24'\\n\",\n",
       " \"'august 16 1813', '1813-08-16'\\n\",\n",
       " \"'4 may 1732', '1732-05-04'\\n\",\n",
       " \"'sep 24 1582', '1582-09-24'\\n\",\n",
       " \"'thu 26 oct 2028', '2028-10-26'\\n\",\n",
       " \"'january 18 2067', '2067-01-18'\\n\",\n",
       " \"'14 june 1733', '1733-06-14'\\n\",\n",
       " \"'october 11 1898', '1898-10-11'\\n\",\n",
       " \"'25 april 1528', '1528-04-25'\\n\",\n",
       " \"'12 december 1988', '1988-12-12'\\n\",\n",
       " \"'wednesday june 18 1547', '1547-06-18'\\n\",\n",
       " \"'7/25/85', '1985-07-25'\\n\",\n",
       " \"'8 november 1874', '1874-11-08'\\n\",\n",
       " \"'june 5 1978', '1978-06-05'\\n\",\n",
       " \"'july 15 1534', '1534-07-15'\\n\",\n",
       " \"'feb 18 2020', '2020-02-18'\\n\",\n",
       " \"'sunday 17 november 1709', '1709-11-17'\\n\",\n",
       " \"'wednesday 1633 20 04', '1633-04-20'\\n\",\n",
       " \"'10/25/08', '1808-10-25'\\n\",\n",
       " \"'wednesday 1737 11 09', '1737-09-11'\\n\",\n",
       " \"'wed 11 aug 1599', '1599-08-11'\\n\",\n",
       " \"'31 october 1671', '1671-10-31'\\n\",\n",
       " \"'3 may 1610', '1610-05-03'\\n\",\n",
       " \"'jul 23 1681', '1681-07-23'\\n\",\n",
       " \"'6 march 1633', '1633-03-06'\\n\",\n",
       " \"'29 march 1944', '1944-03-29'\\n\",\n",
       " \"'4/27/94', '1894-04-27'\\n\",\n",
       " \"'24 august 2051', '2051-08-24'\\n\",\n",
       " \"'18 nov 1993', '1993-11-18'\\n\",\n",
       " \"'september 25 1978', '1978-09-25'\\n\",\n",
       " \"'august 3 1691', '1691-08-03'\\n\",\n",
       " \"'19 oct 1754', '1754-10-19'\\n\",\n",
       " \"'28 november 1995', '1995-11-28'\\n\",\n",
       " \"'saturday october 27 1691', '1691-10-27'\\n\",\n",
       " \"'20/07/1973', '1973-07-20'\\n\",\n",
       " \"'february 22 1636', '1636-02-22'\\n\",\n",
       " \"'november 5 2004', '2004-11-05'\\n\",\n",
       " \"'may 17 1995', '1995-05-17'\\n\",\n",
       " \"'6 dec 1793', '1793-12-06'\\n\",\n",
       " \"'23 december 1558', '1558-12-23'\\n\",\n",
       " \"'may 13 1712', '1712-05-13'\\n\",\n",
       " \"'9 november 1763', '1763-11-09'\\n\",\n",
       " \"'2 may 1685', '1685-05-02'\\n\",\n",
       " \"'february 1 1987', '1987-02-01'\\n\",\n",
       " \"'3/2/57', '2057-03-02'\\n\",\n",
       " \"'march 26 1573', '1573-03-26'\\n\",\n",
       " \"'15 november 2006', '2006-11-15'\\n\",\n",
       " \"'august 10 1664', '1664-08-10'\\n\",\n",
       " \"'march 24 1687', '1687-03-24'\\n\",\n",
       " \"'november 20 1621', '1621-11-20'\\n\",\n",
       " \"'jul 7 2059', '2059-07-07'\\n\",\n",
       " \"'21 january 1626', '1626-01-21'\\n\",\n",
       " \"'thursday november 17 1898', '1898-11-17'\\n\",\n",
       " \"'tuesday 24 april 1725', '1725-04-24'\\n\",\n",
       " \"'monday 8 april 1720', '1720-04-08'\\n\",\n",
       " \"'25 november 2044', '2044-11-25'\\n\",\n",
       " \"'22/02/1599', '1599-02-22'\\n\",\n",
       " \"'friday march 28 1692', '1692-03-28'\\n\",\n",
       " \"'november 11 2010', '2010-11-11'\\n\",\n",
       " \"'sun 3 sep 1916', '1916-09-03'\\n\",\n",
       " \"'mon 1663 5 november', '1663-11-05'\\n\",\n",
       " \"'sat 26 may 1984', '1984-05-26'\\n\",\n",
       " \"'1690 7 jul', '1690-07-07'\\n\",\n",
       " \"'oct 9 1529', '1529-10-09'\\n\",\n",
       " \"'aug 26 1910', '1910-08-26'\\n\",\n",
       " \"'sep 9 1681', '1681-09-09'\\n\",\n",
       " \"'sunday 14 september 1800', '1800-09-14'\\n\",\n",
       " \"'friday september 21 1877', '1877-09-21'\\n\",\n",
       " \"'15 july 1589', '1589-07-15'\\n\",\n",
       " \"'24/03/1608', '1608-03-24'\\n\",\n",
       " \"'jun 26 1783', '1783-06-26'\\n\",\n",
       " \"'may 6 1922', '1922-05-06'\\n\",\n",
       " \"'thu 1652 28 march', '1652-03-28'\\n\",\n",
       " \"'september 11 2050', '2050-09-11'\\n\",\n",
       " \"'mar 23 1996', '1996-03-23'\\n\",\n",
       " \"'16 may 1727', '1727-05-16'\\n\",\n",
       " \"'october 19 1962', '1962-10-19'\\n\",\n",
       " \"'thursday february 22 1540', '1540-02-22'\\n\",\n",
       " \"'friday 2017 24 03', '2017-03-24'\\n\",\n",
       " \"'wednesday june 1 1791', '1791-06-01'\\n\",\n",
       " \"'23/10/1721', '1721-10-23'\\n\",\n",
       " \"'sunday may 16 1604', '1604-05-16'\\n\",\n",
       " \"'sat 1772 5 december', '1772-12-05'\\n\",\n",
       " \"'12 mar 1636', '1636-03-12'\\n\",\n",
       " \"'jan 13 1744', '1744-01-13'\\n\",\n",
       " \"'20 may 1737', '1737-05-20'\\n\",\n",
       " \"'november 11 1774', '1774-11-11'\\n\",\n",
       " \"'dec 10 1767', '1767-12-10'\\n\",\n",
       " \"'aug 25 1663', '1663-08-25'\\n\",\n",
       " \"'2/8/96', '1696-02-08'\\n\",\n",
       " \"'25 july 1923', '1923-07-25'\\n\",\n",
       " \"'november 13 1918', '1918-11-13'\\n\",\n",
       " \"'13 june 2003', '2003-06-13'\\n\",\n",
       " \"'sun 15 sep 1963', '1963-09-15'\\n\",\n",
       " \"'monday april 28 1636', '1636-04-28'\\n\",\n",
       " \"'sunday august 16 1992', '1992-08-16'\\n\",\n",
       " \"'may 21 1690', '1690-05-21'\\n\",\n",
       " \"'sep 16 1624', '1624-09-16'\\n\",\n",
       " \"'march 29 1831', '1831-03-29'\\n\",\n",
       " \"'1773 13 august', '1773-08-13'\\n\",\n",
       " \"'3 april 1776', '1776-04-03'\\n\",\n",
       " \"'september 1 1733', '1733-09-01'\\n\",\n",
       " \"'thursday july 23 1829', '1829-07-23'\\n\",\n",
       " \"'24 june 1610', '1610-06-24'\\n\",\n",
       " \"'mon 2036 7 april', '2036-04-07'\\n\",\n",
       " \"'dec 19 1935', '1935-12-19'\\n\",\n",
       " \"'1751 11 august', '1751-08-11'\\n\",\n",
       " \"'jan 19 1868', '1868-01-19'\\n\",\n",
       " \"'25 january 1597', '1597-01-25'\\n\",\n",
       " \"'wednesday december 13 1989', '1989-12-13'\\n\",\n",
       " \"'june 22 2028', '2028-06-22'\\n\",\n",
       " \"'15/10/1970', '1970-10-15'\\n\",\n",
       " \"'november 27 1965', '1965-11-27'\\n\",\n",
       " \"'tue 6 feb 1720', '1720-02-06'\\n\",\n",
       " \"'sunday april 23 1854', '1854-04-23'\\n\",\n",
       " \"'1/1/45', '1545-01-01'\\n\",\n",
       " \"'december 17 1949', '1949-12-17'\\n\",\n",
       " \"'1831 20 march', '1831-03-20'\\n\",\n",
       " \"'15/09/2065', '2065-09-15'\\n\",\n",
       " \"'november 8 1662', '1662-11-08'\\n\",\n",
       " \"'december 17 1861', '1861-12-17'\\n\",\n",
       " \"'tuesday september 28 2010', '2010-09-28'\\n\",\n",
       " \"'wednesday june 12 1697', '1697-06-12'\\n\",\n",
       " \"'october 31 2028', '2028-10-31'\\n\",\n",
       " \"'sunday 25 january 1559', '1559-01-25'\\n\",\n",
       " \"'1854 10 november', '1854-11-10'\\n\",\n",
       " \"'may 15 1798', '1798-05-15'\\n\",\n",
       " \"'1590 18 jan', '1590-01-18'\\n\",\n",
       " \"'1548 31 aug', '1548-08-31'\\n\",\n",
       " \"'14 july 1597', '1597-07-14'\\n\",\n",
       " \"'1572 18 may', '1572-05-18'\\n\",\n",
       " \"'1728 21 feb', '1728-02-21'\\n\",\n",
       " \"'1999 30 mar', '1999-03-30'\\n\",\n",
       " \"'january 29 1911', '1911-01-29'\\n\",\n",
       " \"'1913 6 apr', '1913-04-06'\\n\",\n",
       " \"'10/04/1813', '1813-04-10'\\n\",\n",
       " \"'october 8 1715', '1715-10-08'\\n\",\n",
       " \"'6 february 1861', '1861-02-06'\\n\",\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Assignment2_train.txt','r') as f:\n",
    "    x = f.readlines()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c0ab06b-05e1-4a5b-bfa8-9c233c85236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ' Hello world ' '\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "text = \" ' Hello world ' '\"\n",
    "print(text)\n",
    "stripped_chars = set([\"'\", \" \"])\n",
    "stripped_chars_str = \"' \\\"\\\\n\"\n",
    "text = text.strip(stripped_chars_str)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd14416b-7129-4dc3-a60f-ca4873ed777e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ', \"'\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripped_chars = set([\"'\", \" \"])\n",
    "stripped_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89b0d3bd-4520-425d-bc4b-b34ce3c79c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'march 8 1758'\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].split(\",\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "982eac88-0719-45fc-a292-74425f272e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "march 8 1758\n"
     ]
    }
   ],
   "source": [
    "temp = x[0].split(\",\")[0]\n",
    "print(temp.strip(stripped_chars_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef4f37f2-88ac-4922-b23c-2aba9e992184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(temp.strip(stripped_chars_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f2979a8-cfa6-45ec-8283-15ad4200c69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['march 8 1758', '1758-03-08'], ['17 february 1709', '1709-02-17'], ['13 may 1786', '1786-05-13'], ['17 june 1626', '1626-06-17'], ['july 25 1851', '1851-07-25']]\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "for data in x:\n",
    "    data_hd = data.split(\",\")[0]\n",
    "    data_md = data.split(\",\")[1]\n",
    "    data_hd = data_hd.strip(stripped_chars_str)\n",
    "    data_md = data_md.strip(\"\\n\")\n",
    "    data_md = data_md.strip(stripped_chars_str)\n",
    "    y.append([data_hd, data_md])\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf5387aa-3531-4548-b6ed-60bb60acd7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>', 'y', ' ', '/', '6', 'v', 'w', '<unk>', 'i', '2', 'f', 'g', 'n', 'l', '9', 'd', 's', 'r', 'o', '-', 'u', 'h', '5', '3', 'e', '8', 'b', 'j', '<eos>', 'a', '4', 'm', 'c', '1', '<sos>', 'p', '0', 't', '7'}\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for dates in y:\n",
    "    for char in dates[0]:\n",
    "        vocab.add(char)\n",
    "    for char in dates[1]:\n",
    "        vocab.add(char)\n",
    "vocab.add(\"<sos>\")\n",
    "vocab.add(\"<eos>\")\n",
    "vocab.add(\"<pad>\")\n",
    "vocab.add(\"<unk>\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a4cc7d3-74df-4476-9bb2-c60e1604adbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', 'y', ' ', '/', '6', 'v', 'w', '<unk>', 'i', '2', 'f', 'g', 'n', 'l', '9', 'd', 's', 'r', 'o', '-', 'u', 'h', '5', '3', 'e', '8', 'b', 'j', '<eos>', 'a', '4', 'm', 'c', '1', '<sos>', 'p', '0', 't', '7']\n"
     ]
    }
   ],
   "source": [
    "vocab_list = list(vocab)\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af4edfd5-71ad-45b4-8383-f38909b1f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "017dbd6e-310f-44e7-8971-c3fb95e28dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '-', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '<eos>', '<pad>', '<sos>', '<unk>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y']\n"
     ]
    }
   ],
   "source": [
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3bf1e96-0602-4524-a6f1-40843e27eabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"Rajkamal is a good boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ba0ffed-4321-4cef-89d8-3ff7901a76db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating max_seq_len possible of human dates\n",
    "mx = -1\n",
    "for dates in y:\n",
    "    mx = max(mx, len(dates[0]))\n",
    "mx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc89cb8-7174-4408-ad8c-c61c4aa031fa",
   "metadata": {},
   "source": [
    "So, my transformer architecture's max_seq_len is going to be 32.\n",
    "Decoder's output max_seq_len need not be equal to that of encoder's, in my case, as the output is always going to be YYYY-MM-DD so, I will be generating 11 characters in the output via decoder (one extra character for end token - can skip it, depends, check it later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47336f50-ecd5-4c90-a278-b646e5d224fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '-': 1, '/': 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, '<eos>': 13, '<pad>': 14, '<sos>': 15, '<unk>': 16, 'a': 17, 'b': 18, 'c': 19, 'd': 20, 'e': 21, 'f': 22, 'g': 23, 'h': 24, 'i': 25, 'j': 26, 'l': 27, 'm': 28, 'n': 29, 'o': 30, 'p': 31, 'r': 32, 's': 33, 't': 34, 'u': 35, 'v': 36, 'w': 37, 'y': 38}\n"
     ]
    }
   ],
   "source": [
    "ctoi = {ch:i for i,ch in enumerate(vocab_list)}\n",
    "print(ctoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7527f78f-2406-4f57-a864-1457ace6a46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ' ', 1: '-', 2: '/', 3: '0', 4: '1', 5: '2', 6: '3', 7: '4', 8: '5', 9: '6', 10: '7', 11: '8', 12: '9', 13: '<eos>', 14: '<pad>', 15: '<sos>', 16: '<unk>', 17: 'a', 18: 'b', 19: 'c', 20: 'd', 21: 'e', 22: 'f', 23: 'g', 24: 'h', 25: 'i', 26: 'j', 27: 'l', 28: 'm', 29: 'n', 30: 'o', 31: 'p', 32: 'r', 33: 's', 34: 't', 35: 'u', 36: 'v', 37: 'w', 38: 'y'}\n"
     ]
    }
   ],
   "source": [
    "itoc = {i:ch for i,ch in enumerate(vocab_list)}\n",
    "print(itoc)\n",
    "#will be needed while decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f24bd-63e5-4e04-b31f-7700426b1122",
   "metadata": {},
   "source": [
    "Standard word embedding dimensions I encountered during word2vec, glove and fasttext was around 300, whereas for all-mpnet-base-v2, it was 768, so, accordingly scaling for character embedding dimensions - I believe 100 dim is good starting point for now!<br>\n",
    "So, my d_model = 100!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e054662a-c446-4d8f-937d-1584090a7de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebf1351e-a3dd-4da7-9127-0a0889e14be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 100])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "tensor([-1.0646,  1.6308,  1.5996, -1.9984,  0.2420, -0.6331,  0.1340, -1.5962,\n",
      "        -0.5247, -1.1418,  0.0591,  1.5399,  0.3686, -0.6437,  0.0927, -0.5058,\n",
      "         0.9930, -1.8410,  0.0176,  1.2356, -1.9174,  0.3533,  0.6007, -2.1403,\n",
      "         1.8278, -0.5869, -1.9603, -1.8965, -0.4448, -0.4983, -0.5537,  0.4554,\n",
      "         0.9840, -0.4013,  0.1496, -0.6410, -1.4493, -0.3307,  0.5997, -0.0778,\n",
      "         0.3949,  0.5148,  0.1501, -0.4114, -0.0235,  0.0398, -0.7414,  1.7041,\n",
      "        -1.6444,  1.1851,  0.7185,  1.1084, -1.2935,  0.8462,  0.7800, -1.2825,\n",
      "         1.4343, -0.1464,  0.7419, -0.6069,  0.4680, -0.5452, -0.3891, -1.1141,\n",
      "         1.2661, -0.6330, -0.4372,  0.2416, -1.2522,  1.0115,  1.0558,  2.1782,\n",
      "        -1.7518,  1.2241, -0.0327, -0.0319, -1.0078,  0.3169, -1.6777, -2.2731,\n",
      "         1.3643,  0.4536,  0.8712,  0.5663,  0.1144,  0.7137, -0.6666, -1.5259,\n",
      "        -0.4213, -1.2831, -0.1430,  0.0921, -0.6033,  0.3410, -0.9406,  0.8807,\n",
      "         0.0264, -1.3283,  0.9637,  0.5073], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):#nn.Module is needed to have the powers of nn i.e. BACKPROPAGATION, Computation graph, etc\n",
    "    def __init__(self):#constructor\n",
    "        super().__init__() #calling constructor of nn.Module to have all it's functionalities available to me\n",
    "        self.embedding = nn.Embedding(39, 100, padding_idx=14) \n",
    "        #self.embedding[14] = torch.zeroes()\n",
    "        with torch.no_grad():\n",
    "            self.embedding.weight[14] = torch.zeros(100)\n",
    "\n",
    "encoder = Encoder()\n",
    "#print(encoder.embedding.shape)\n",
    "print(encoder.embedding.weight.shape)\n",
    "#print(encoder.embedding[14])\n",
    "print(encoder.embedding.weight[14])\n",
    "print(encoder.embedding.weight[1])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5b057111-92da-4cd2-a7d5-813bc79eae5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0984,  1.3883, -0.7145,  0.6942],\n",
      "        [-0.9190,  1.5489, -0.5259,  0.9685],\n",
      "        [-2.3442,  0.5018, -0.8843,  0.9437],\n",
      "        [ 0.1609, -0.0823,  0.7944,  1.3473]])\n",
      "tensor([[-0.9190,  1.5489, -0.5259,  0.9685],\n",
      "        [-2.3442,  0.5018, -0.8843,  0.9437]])\n",
      "tensor([-0.9190,  1.5489, -0.5259,  0.9685])\n",
      "torch.Size([4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3883,  1.5489,  0.5018, -0.0823])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.randn(4,4)\n",
    "rows = [1,2]\n",
    "print(temp)\n",
    "print(temp[rows])\n",
    "print(temp[:][1])\n",
    "print(temp.shape)\n",
    "temp[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "82c1b13e-2f5a-4417-b364-c21df0e6c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 32\n",
    "EMBEDDING_DIM = 128 #100 changed, since, I want it to be divisible by 4\n",
    "#EMBEDDING_DIM is the d_model we talked about while discussing architecture\n",
    "PAD_IDX = ctoi[\"<pad>\"]\n",
    "VOCAB_SIZE = len(vocab_list)\n",
    "ATTENTION_HEADS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3de26238-dc1a-4574-bf22-2bf4b4c0c941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.randint(0,10,(3,4))\n",
    "temp *= 10\n",
    "temp\n",
    "\n",
    "import math\n",
    "math.sqrt(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8bc938a9-9eaf-4e28-b336-b68e4db20e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0323, 0.2477, 0.7201],\n",
       "        [0.6454, 0.2040, 0.1507]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.randn(2,3)\n",
    "softmax = nn.Softmax(dim=1)#across row - sum = 1\n",
    "softmax(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4da10e49-6f4b-4909-bf38-f2f53e9cbc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2437,  0.3785,  1.3521, -1.0081])\n",
      "tensor([ 0.6855, -0.7513,  0.0704,  0.1691])\n",
      "torch.Size([8])\n",
      "tensor([ 0.2437,  0.3785,  1.3521, -1.0081,  0.6855, -0.7513,  0.0704,  0.1691])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Two 1D tensors\n",
    "temp = torch.randn(4)\n",
    "temp2 = torch.randn(4)\n",
    "print(temp)\n",
    "print(temp2)\n",
    "\n",
    "# Concatenate them along dimension 0\n",
    "result = torch.cat((temp, temp2), dim=0)\n",
    "\n",
    "print(result.shape)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "b9904927-cb36-484f-bb28-32646d229460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 3],\n",
      "        [2, 0]])\n",
      "tensor([[5, 2],\n",
      "        [8, 3]])\n",
      "tensor([[7, 3],\n",
      "        [2, 0],\n",
      "        [5, 2],\n",
      "        [8, 3]])\n",
      "torch.Size([4, 2])\n",
      "tensor([[[7, 3],\n",
      "         [2, 0]],\n",
      "\n",
      "        [[5, 2],\n",
      "         [8, 3]]])\n",
      "torch.Size([2, 2, 2])\n",
      "tensor([[0, 9],\n",
      "        [4, 0]])\n",
      "tensor([[2, 5],\n",
      "        [7, 5]])\n",
      "tensor([[7, 3],\n",
      "        [2, 0],\n",
      "        [5, 2],\n",
      "        [8, 3],\n",
      "        [0, 9],\n",
      "        [4, 0],\n",
      "        [2, 5],\n",
      "        [7, 5]])\n",
      "torch.Size([8, 2])\n",
      "tensor([[[7, 3],\n",
      "         [2, 0]],\n",
      "\n",
      "        [[5, 2],\n",
      "         [8, 3]],\n",
      "\n",
      "        [[0, 9],\n",
      "         [4, 0]],\n",
      "\n",
      "        [[2, 5],\n",
      "         [7, 5]]])\n",
      "torch.Size([4, 2, 2])\n",
      "tensor([[3, 6],\n",
      "        [2, 4]])\n",
      "TEMP10:  tensor([[[27, 54],\n",
      "         [ 6, 12]],\n",
      "\n",
      "        [[19, 38],\n",
      "         [30, 60]],\n",
      "\n",
      "        [[18, 36],\n",
      "         [12, 24]],\n",
      "\n",
      "        [[16, 32],\n",
      "         [31, 62]]])\n",
      "torch.Size([4, 2, 2])\n",
      "TEMP11:  tensor([[[27, 54],\n",
      "         [ 6, 12]],\n",
      "\n",
      "        [[19, 38],\n",
      "         [30, 60]],\n",
      "\n",
      "        [[18, 36],\n",
      "         [12, 24]],\n",
      "\n",
      "        [[16, 32],\n",
      "         [31, 62]]])\n",
      "torch.Size([4, 2, 2])\n",
      "tensor([[[33,  9],\n",
      "         [22,  6]],\n",
      "\n",
      "        [[63, 24],\n",
      "         [42, 16]],\n",
      "\n",
      "        [[24, 27],\n",
      "         [16, 18]],\n",
      "\n",
      "        [[48, 45],\n",
      "         [32, 30]]])\n",
      "torch.Size([4, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "#FOR PARALLELIZING computations in multi-head attention across n heads\n",
    "\n",
    "temp = torch.randint(0,10,(2,2))\n",
    "print(temp)\n",
    "temp2 = torch.randint(0,10,(2,2))\n",
    "print(temp2)\n",
    "temp3 = torch.cat((temp, temp2), dim = 0)\n",
    "print(temp3)\n",
    "print(temp3.shape)\n",
    "#2*2 matrix and another 2*2 matrix - concatenate them to get 2 * 2*2 matrix\n",
    "temp4 = temp3.view(-1,2,2)\n",
    "print(temp4)\n",
    "print(temp4.shape)\n",
    "\n",
    "\n",
    "temp5 = torch.randint(0,10, (2,2))\n",
    "print(temp5)\n",
    "temp6 = torch.randint(0,10,(2,2))\n",
    "print(temp6)\n",
    "\n",
    "temp7 = torch.cat((temp, temp2, temp5, temp6), dim = 0)\n",
    "print(temp7)\n",
    "print(temp7.shape)\n",
    "\n",
    "temp8 = temp7.view(-1, 2, 2)\n",
    "print(temp8)\n",
    "print(temp8.shape)\n",
    "\n",
    "temp9 = torch.randint(0,10, (2,2))\n",
    "print(temp9)\n",
    "temp10 = temp8 @ temp9\n",
    "print(\"TEMP10: \",temp10)\n",
    "print(temp10.shape)\n",
    "\n",
    "temp9 = temp9.view(-1, 2, 2)\n",
    "temp11 = temp8 @ temp9\n",
    "print(\"TEMP11: \",temp11)\n",
    "print(temp11.shape)\n",
    "\n",
    "temp12 = temp9 @ temp8\n",
    "print(temp12)\n",
    "print(temp12.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5058aea0-aa9c-4ed9-b913-46e0488eac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, max_seq_len = MAX_SEQ_LEN, ip_embedding_dim = EMBEDDING_DIM, proj_embedding_dim = EMBEDDING_DIM):\n",
    "        super().__init__()\n",
    "        #self.W_q = torch.randn(max_seq_len, embedding_dim)\n",
    "        #self.W_k = torch.randn(max_seq_len, embedding_dim)\n",
    "        #self.W_v = torch.randn(max_seq_len, embedding_dim)\n",
    "        self.W_q = torch.randn(ip_embedding_dim, proj_embedding_dim)\n",
    "        self.W_k = torch.randn(ip_embedding_dim, proj_embedding_dim)\n",
    "        self.W_v = torch.randn(ip_embedding_dim, proj_embedding_dim)\n",
    "\n",
    "    def forward(self, token_embeddings):\n",
    "        Q = token_embeddings @ self.W_q\n",
    "        K = token_embeddings @ self.W_k\n",
    "        V = token_embeddings @ self.W_v\n",
    "        print(K.shape[1])\n",
    "        Q_K = (Q @ K.T)/math.sqrt(K.shape[1])\n",
    "\n",
    "        #My V matrix: is 32 * 100, so I will have to do across-row softmax, if it was 100 * 32, then, across-col softmax would have worked\n",
    "        #NOTE: dim = 0 => row dim, dim = 1 => \n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        softmax_op = softmax(Q_K)\n",
    "\n",
    "        print(\"SOFTMAX OP: \",softmax_op.shape)\n",
    "\n",
    "        output = softmax_op @ V\n",
    "\n",
    "        print(output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "        #print(Q_K.shape)\n",
    "        \n",
    "        #print(Q.shape)\n",
    "        #print(K.shape)\n",
    "        #print(V.shape)\n",
    "        #print(self.W_q.shape)\n",
    "        #print(token_embeddings.shape)\n",
    "        #return token_embeddings\n",
    "\n",
    "class Multi_Head_Attention(nn.Module):\n",
    "    def __init__(self, heads = ATTENTION_HEADS, ip_embedding_dim = EMBEDDING_DIM, max_seq_len = MAX_SEQ_LEN):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        print(\"REDUCED DIMENSION: \", int(ip_embedding_dim/self.heads))\n",
    "        self.attention_heads = [Attention(ip_embedding_dim = int(ip_embedding_dim/self.heads), proj_embedding_dim = int(ip_embedding_dim/self.heads)) for i in range(heads)]\n",
    "        self.W_qs = [head.W_q for head in self.attention_heads]#learnable\n",
    "        self.W_ks = [head.W_k for head in self.attention_heads]#learnable\n",
    "        self.W_vs = [head.W_v for head in self.attention_heads]#learnable\n",
    "        self.ip_embedding_dim = ip_embedding_dim\n",
    "        self.proj_embedding_dim = int(ip_embedding_dim/heads)\n",
    "        self.token_projection = nn.Linear(ip_embedding_dim, self.proj_embedding_dim)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.linear = nn.Linear(ip_embedding_dim, ip_embedding_dim)\n",
    "        #self.mask = torch.zeros(max_seq_len * max_seq_len)#NOT LEARNABLE\n",
    "        #self.mask[:, -num_padded_tokens] = -float('inf')\n",
    "\n",
    "    def forward(self, token_embeddings, num_padded_tokens):\n",
    "        #I was thinking of computing contextually aware repr from each of the attention heads one-by-one and then concatenating them\n",
    "        #This will work and it's implementation is also pretty striaghtforward, but this defeats the purpose of parallelization\n",
    "        #which can be achieved across the attention heads\n",
    "        W_qs_ = torch.cat(tuple(self.W_qs), dim = 0)\n",
    "        W_ks_ = torch.cat(tuple(self.W_ks), dim = 0)\n",
    "        W_vs_ = torch.cat(tuple(self.W_vs), dim = 0)\n",
    "        print(\"EYLLO\")\n",
    "        print(W_qs_.shape)\n",
    "        print(W_ks_.shape)\n",
    "        print(W_vs_.shape)\n",
    "\n",
    "        print(\"EYLLOSLKDJFLS\")\n",
    "\n",
    "        W_qs_ = W_qs_.view(-1, self.proj_embedding_dim, self.proj_embedding_dim)\n",
    "        W_ks_ = W_ks_.view(-1, self.proj_embedding_dim, self.proj_embedding_dim)\n",
    "        W_vs_ = W_vs_.view(-1, self.proj_embedding_dim, self.proj_embedding_dim)\n",
    "\n",
    "        print(W_qs_.shape)\n",
    "        print(W_ks_.shape)\n",
    "        print(W_vs_.shape)\n",
    "\n",
    "        print(token_embeddings.shape)\n",
    "\n",
    "        token_embeddings_proj = self.token_projection(token_embeddings)\n",
    "\n",
    "        print(token_embeddings_proj.shape)\n",
    "\n",
    "        print(\"W_qs_ shape: \",W_qs_.shape)\n",
    "        print(\"token_embeddings_proj shape: \",token_embeddings_proj.shape)\n",
    "        print(\"token_embeddings_proj transpose shape: \",token_embeddings_proj.T.shape)\n",
    "\n",
    "        #Qs = W_qs_ @ token_embeddings_proj.T\n",
    "        #print(Qs.shape)\n",
    "        #Ks = W_ks_ @ token_embeddings_proj.T\n",
    "        #print(Ks.shape)\n",
    "        squeezed = token_embeddings_proj.T.unsqueeze(0)\n",
    "        print(\"token_embeddings_proj.unsqueeze: \", squeezed.shape )\n",
    "        Qs = torch.matmul(W_qs_, squeezed)\n",
    "        print(Qs.shape)\n",
    "        Ks = torch.matmul(W_ks_, squeezed)\n",
    "        print(Ks.shape)\n",
    "        Vs = torch.matmul(W_vs_, squeezed)\n",
    "        print(Vs.shape)\n",
    "\n",
    "        Qs_Ks = torch.bmm(Qs.transpose(1,2), Ks)/math.sqrt(Ks.shape[1])\n",
    "\n",
    "        print(Qs_Ks.shape)\n",
    "\n",
    "        mask = torch.zeros(self.max_seq_len, self.max_seq_len)\n",
    "        #softmax_op = nn.functional.softmax(Qs_Ks, dim = 1)\n",
    "        #mask[:, -num_padded_tokens:] = -float('inf')\n",
    "        if num_padded_tokens != 0:\n",
    "            mask[-num_padded_tokens:,:] = -float('inf')\n",
    "        print('MASK', mask)\n",
    "        print(\"MASK SHAPE: \",mask.shape)\n",
    "        softmax_op = nn.functional.softmax(Qs_Ks + mask, dim = 1) #With mask added\n",
    "        print('SOFTMAX OUTPUT: ',softmax_op)\n",
    "\n",
    "        print(softmax_op.shape)\n",
    "\n",
    "        output = torch.bmm(softmax_op, Vs.transpose(1,2))\n",
    "\n",
    "        print(output.shape)\n",
    "\n",
    "        head_outputs = [head_op for head_op in output]\n",
    "\n",
    "        final_output = torch.cat(tuple(head_outputs), dim = 1)\n",
    "\n",
    "        print(final_output.shape)\n",
    "\n",
    "        #YOU CONCATENATED THE OUTPUS OF EACH HEAD, BUT DIDN'T APPLY A LINEAR LAYER AFTERWORDS, directly went to Add Norm Layer\n",
    "        final_output = self.linear(final_output)\n",
    "        #return final_output\n",
    "        return final_output + token_embeddings #Add Layer\n",
    "\n",
    "        #Qs = W_qs @ token_embeddings\n",
    "        \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = torch.ones(d_model)#Should set requires_grad = True\n",
    "        self.beta = torch.zeros(d_model)#Should set requires_grad = True\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, token_embeddings):#max_seq_len * d_model\n",
    "        token_embeddings_mean = torch.mean(token_embeddings, dim=1, keepdim=True)\n",
    "        token_embeddings_var = ((token_embeddings - token_embeddings_mean)**2)/token_embeddings.shape[1]\n",
    "        token_embeddings_normalized = (token_embeddings - token_embeddings_mean)/torch.sqrt(token_embeddings_var + self.eps)\n",
    "        scaled_shifted_token_embeddings = self.gamma * token_embeddings_normalized + self.beta\n",
    "        return scaled_shifted_token_embeddings\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.proj_space_dim = d_model*2\n",
    "        self.lin1 = nn.Linear(d_model, self.proj_space_dim)\n",
    "        self.lin2 = nn.Linear(self.proj_space_dim, d_model)\n",
    "\n",
    "    def forward(self,token_embeddings):\n",
    "        transformed1 = self.lin1(token_embeddings)\n",
    "        nonlin_transformed = torch.tanh(transformed1)\n",
    "        transformed2 = self.lin2(nonlin_transformed)\n",
    "        #return transformed2 \n",
    "        return transformed2 + token_embeddings #Add Layer\n",
    "        \n",
    "\n",
    "class Encoder(nn.Module):#nn.Module is needed to have the powers of nn i.e. BACKPROPAGATION, Computation graph, etc\n",
    "    def __init__(self, vocab_size = VOCAB_SIZE, max_seq_len = MAX_SEQ_LEN, embedding_dim = EMBEDDING_DIM):#constructor\n",
    "        super().__init__() #calling constructor of nn.Module to have all it's functionalities available to me\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_IDX) \n",
    "        #self.embedding[14] = torch.zeroes()\n",
    "        with torch.no_grad():\n",
    "            self.embedding.weight[PAD_IDX] = torch.zeros(embedding_dim)\n",
    "        self.attention = Attention(max_seq_len, embedding_dim)\n",
    "        self.mul_attention = Multi_Head_Attention()\n",
    "        self.layer_norm = LayerNorm(embedding_dim)\n",
    "        self.ffnn = FFNN(embedding_dim)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, tokens, padded_tokens=0):#I will need just token_ids of words as input\n",
    "        #ALSO NEED TO KEEP TRACK NUMBER OF TOKENS THAT WERE PADDED - THEY WILL HELP ME TO DEFINE THE MASKING MATRIX\n",
    "        print(\"TOKENS TOKENS: \",tokens)\n",
    "        #padded_tokens = self.max_seq_len - len(tokens)\n",
    "        print(\"PADDED TOKENS: \",padded_tokens)\n",
    "        #while len(tokens) < self.max_seq_len:\n",
    "        #    tokens.append(PAD_IDX)\n",
    "        token_embeddings = self.embedding.weight[tokens]\n",
    "        #UPDATE token_embeddings with positional encodings HERE\n",
    "        \n",
    "        #contextually_aware_representations = self.attention(token_embeddings)\n",
    "        contextually_aware_representations = self.mul_attention(token_embeddings = token_embeddings, num_padded_tokens = padded_tokens)\n",
    "        print(\"CONTEXTUALLY AWARE REPR: \", contextually_aware_representations)\n",
    "        if contextually_aware_representations != None:\n",
    "            print(\"shape: \", contextually_aware_representations.shape)\n",
    "        layer_normed_car = self.layer_norm(contextually_aware_representations)\n",
    "        #return contextually_aware_representations\n",
    "        print(\"layer normed car shape: \",layer_normed_car.shape)\n",
    "        car_after_ffnn = self.ffnn(layer_normed_car)\n",
    "        layer_normed_car_after_ffnn = self.layer_norm(car_after_ffnn)\n",
    "        \n",
    "        #return layer_normed_car\n",
    "        print(\"layer normed car after ffnn shape: \",layer_normed_car_after_ffnn.shape)\n",
    "        return layer_normed_car_after_ffnn\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2e20cf76-cb8e-4501-83cb-54ce2e03ceab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10th November 2000\n",
      "10th november 2000\n",
      "[4, 3, 34, 24, 0, 29, 30, 36, 21, 28, 18, 21, 32, 0, 5, 3, 3, 3, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
      "14\n",
      "REDUCED DIMENSION:  64\n",
      "TOKENS TOKENS:  [4, 3, 34, 24, 0, 29, 30, 36, 21, 28, 18, 21, 32, 0, 5, 3, 3, 3, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
      "PADDED TOKENS:  14\n",
      "EYLLO\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n",
      "EYLLOSLKDJFLS\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 64])\n",
      "W_qs_ shape:  torch.Size([2, 64, 64])\n",
      "token_embeddings_proj shape:  torch.Size([32, 64])\n",
      "token_embeddings_proj transpose shape:  torch.Size([64, 32])\n",
      "token_embeddings_proj.unsqueeze:  torch.Size([1, 64, 32])\n",
      "torch.Size([2, 64, 32])\n",
      "torch.Size([2, 64, 32])\n",
      "torch.Size([2, 64, 32])\n",
      "torch.Size([2, 32, 32])\n",
      "MASK tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]])\n",
      "MASK SHAPE:  torch.Size([32, 32])\n",
      "SOFTMAX OUTPUT:  tensor([[[3.0151e-15, 4.1740e-28, 3.9138e-31,  ..., 1.1059e-02,\n",
      "          1.1059e-02, 1.1059e-02],\n",
      "         [2.4228e-28, 2.8298e-28, 1.2198e-14,  ..., 6.6169e-03,\n",
      "          6.6169e-03, 6.6169e-03],\n",
      "         [4.0944e-03, 1.6955e-17, 5.8706e-18,  ..., 6.6842e-02,\n",
      "          6.6842e-02, 6.6842e-02],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[1.3945e-06, 2.2218e-33, 3.1470e-28,  ..., 6.2129e-03,\n",
      "          6.2129e-03, 6.2129e-03],\n",
      "         [3.3118e-17, 2.4530e-34, 8.6061e-09,  ..., 2.2615e-02,\n",
      "          2.2615e-02, 2.2615e-02],\n",
      "         [1.0452e-19, 3.2206e-35, 4.1570e-17,  ..., 6.3689e-02,\n",
      "          6.3689e-02, 6.3689e-02],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([2, 32, 32])\n",
      "torch.Size([2, 32, 64])\n",
      "torch.Size([32, 128])\n",
      "CONTEXTUALLY AWARE REPR:  tensor([[ 5.9720e-01, -2.9804e+00, -1.7155e+00,  ..., -1.7089e+00,\n",
      "         -5.4995e+00, -2.8834e+00],\n",
      "        [-1.9465e-01, -2.1205e+00,  7.4383e-01,  ...,  6.2534e-01,\n",
      "         -3.8662e-01,  3.9773e-01],\n",
      "        [-2.4447e+00, -6.8404e-01,  8.0081e-01,  ..., -1.1682e-01,\n",
      "         -1.0069e+00,  1.3469e+00],\n",
      "        ...,\n",
      "        [-5.7973e-02, -4.7750e-02, -6.9658e-03,  ...,  7.6638e-02,\n",
      "          5.0414e-02, -4.0928e-03],\n",
      "        [-5.7973e-02, -4.7750e-02, -6.9658e-03,  ...,  7.6638e-02,\n",
      "          5.0414e-02, -4.0928e-03],\n",
      "        [-5.7973e-02, -4.7750e-02, -6.9658e-03,  ...,  7.6638e-02,\n",
      "          5.0414e-02, -4.0928e-03]], grad_fn=<AddBackward0>)\n",
      "shape:  torch.Size([32, 128])\n",
      "layer normed car shape:  torch.Size([32, 128])\n",
      "layer normed car after ffnn shape:  torch.Size([32, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 11.3137, -11.3137, -11.3137,  ..., -11.3137, -11.3137, -11.3137],\n",
       "        [-11.3137, -11.3137,  11.3137,  ...,  11.3137, -11.3137,  11.3137],\n",
       "        [-11.3137, -11.3137,  11.3137,  ...,  11.3137, -11.3137,  11.3137],\n",
       "        ...,\n",
       "        [-11.3137, -11.3137, -11.3137,  ...,  11.3137,  11.3137, -11.3137],\n",
       "        [-11.3137, -11.3137, -11.3137,  ...,  11.3137,  11.3137, -11.3137],\n",
       "        [-11.3137, -11.3137, -11.3137,  ...,  11.3137,  11.3137, -11.3137]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ip = \"10th November 2000\"\n",
    "#sample_op = \"2000-11-10\"\n",
    "def get_ip_token_indices(str):\n",
    "    tokens = []\n",
    "    print(str)\n",
    "    str = str.lower()\n",
    "    print(str)\n",
    "    for ch in str:\n",
    "        tokens.append(ctoi[ch])\n",
    "    padded_tokens = MAX_SEQ_LEN - len(tokens)\n",
    "    while len(tokens) < MAX_SEQ_LEN:\n",
    "        tokens.append(ctoi[\"<pad>\"])\n",
    "    return tokens, padded_tokens\n",
    "sample_ip_tokens, num_padded_tokens = get_ip_token_indices(sample_ip)\n",
    "print(sample_ip_tokens)\n",
    "print(num_padded_tokens)\n",
    "encoder = Encoder()\n",
    "encoder(sample_ip_tokens, num_padded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a537d5c2-9971-4684-bbdf-95ee0864d830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[7, 8, 4],\n",
      "         [5, 5, 1],\n",
      "         [6, 3, 6],\n",
      "         [0, 2, 1]],\n",
      "\n",
      "        [[7, 9, 9],\n",
      "         [6, 9, 2],\n",
      "         [6, 5, 8],\n",
      "         [2, 2, 4]]])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[5, 5, 5],\n",
      "         [8, 7, 8],\n",
      "         [9, 0, 8],\n",
      "         [1, 2, 5]]])\n",
      "torch.Size([1, 4, 3])\n",
      "TEMP2, can transpose be recovered:  tensor([[[5, 5, 5, 8],\n",
      "         [7, 8, 9, 0],\n",
      "         [8, 1, 2, 5]]])\n",
      "TEMP2's transpose recover:  tensor([[[5, 8, 9, 1],\n",
      "         [5, 7, 0, 2],\n",
      "         [5, 8, 8, 5]]]) torch.Size([1, 3, 4])\n",
      "tensor([[[35, 40, 20],\n",
      "         [40, 35,  8],\n",
      "         [54,  0, 48],\n",
      "         [ 0,  4,  5]],\n",
      "\n",
      "        [[35, 45, 45],\n",
      "         [48, 63, 16],\n",
      "         [54,  0, 64],\n",
      "         [ 2,  4, 20]]])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[5, 9, 9, 8],\n",
      "         [4, 6, 8, 1],\n",
      "         [7, 3, 8, 4]]])\n",
      "torch.Size([1, 3, 4])\n",
      "tensor([[[ 95, 123, 159,  80],\n",
      "         [ 52,  78,  93,  49],\n",
      "         [ 84,  90, 126,  75],\n",
      "         [ 15,  15,  24,   6]],\n",
      "\n",
      "        [[134, 144, 207, 101],\n",
      "         [ 80, 114, 142,  65],\n",
      "         [106, 108, 158,  85],\n",
      "         [ 46,  42,  66,  34]]])\n",
      "torch.Size([2, 4, 4])\n",
      "tensor([[[129, 109,  83],\n",
      "         [ 70,  79,  29],\n",
      "         [123, 114,  81]],\n",
      "\n",
      "        [[139, 164, 137],\n",
      "         [ 81, 112,  67],\n",
      "         [141, 167, 145]]])\n"
     ]
    }
   ],
   "source": [
    "temp = torch.randint(0,10,(2,4,3))\n",
    "print(temp)\n",
    "print(temp.shape)\n",
    "temp2 = torch.randint(0,10,(1,4,3))\n",
    "print(temp2)\n",
    "print(temp2.shape)\n",
    "\n",
    "print(\"TEMP2, can transpose be recovered: \",temp2.view(-1,3,4))\n",
    "\n",
    "print(\"TEMP2's transpose recover: \",temp2.transpose(1,2),temp2.transpose(1,2).shape)\n",
    "\n",
    "temp3 = temp * temp2\n",
    "print(temp3)\n",
    "print(temp3.shape)\n",
    "\n",
    "temp4 = torch.randint(0,10,(1,3,4))\n",
    "print(temp4)\n",
    "print(temp4.shape)\n",
    "temp5 = temp @ temp4\n",
    "print(temp5)\n",
    "print(temp5.shape)\n",
    "\n",
    "print(torch.bmm(torch.cat((temp2.transpose(1,2), temp2.transpose(1,2))), temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11dfc828-6f4e-4bff-b539-a39c325f8321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0846, -0.1009,  0.8951,  0.6654],\n",
      "         [ 0.3064,  0.4217, -0.0679,  1.5517],\n",
      "         [-0.2929,  1.3985,  0.5326, -0.3164],\n",
      "         [-1.4899, -0.4913, -0.4774, -0.9882]],\n",
      "\n",
      "        [[ 0.4288, -0.3971, -2.5808,  0.4473],\n",
      "         [ 1.7646,  1.2280, -1.6735, -0.0080],\n",
      "         [-0.4540,  0.1563,  0.6804, -0.2936],\n",
      "         [-1.2752, -0.4248,  0.4525, -0.3085]]])\n",
      "tensor([[[0.3184, 0.1275, 0.4290, 0.2505],\n",
      "         [0.3974, 0.2150, 0.1638, 0.6077],\n",
      "         [0.2183, 0.5711, 0.2985, 0.0938],\n",
      "         [0.0659, 0.0863, 0.1087, 0.0479]],\n",
      "\n",
      "        [[0.1852, 0.1138, 0.0199, 0.3875],\n",
      "         [0.7044, 0.5778, 0.0492, 0.2458],\n",
      "         [0.0766, 0.1978, 0.5183, 0.1847],\n",
      "         [0.0337, 0.1106, 0.4126, 0.1820]]])\n",
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "temp = torch.randn(2,4,4)\n",
    "print(temp)\n",
    "softmax_res = nn.functional.softmax(temp, dim=1)\n",
    "print(softmax_res)\n",
    "print(softmax_res.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f66d59b8-7481-47ad-b8c9-6d6f28c582b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 1],\n",
      "         [1, 0]],\n",
      "\n",
      "        [[4, 1],\n",
      "         [1, 4]]])\n",
      "tensor([[[0, 1, 1, 0],\n",
      "         [4, 1, 1, 4]]])\n",
      "tensor([[[0, 1],\n",
      "         [1, 0]],\n",
      "\n",
      "        [[4, 1],\n",
      "         [1, 4]]])\n",
      "torch.Size([2, 2, 2])\n",
      "tensor([[0, 1, 4, 1],\n",
      "        [1, 0, 1, 4]])\n",
      "tensor([[0, 1, 4, 1],\n",
      "        [1, 0, 1, 4]])\n"
     ]
    }
   ],
   "source": [
    "temp = torch.randint(0,5,(2,2,2))\n",
    "print(temp)\n",
    "print(temp.view(1,2,-1))\n",
    "print(temp.squeeze(0))\n",
    "print(temp.squeeze(0).shape)\n",
    "print(torch.cat((temp[0], temp[1]), dim=1))\n",
    "print(torch.cat(tuple([data for data in temp]), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "861a383f-b2c2-47db-b92c-4d9c567b3a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 0., 3.],\n",
      "        [1., 4., 3., 4.]])\n",
      "torch.Size([2, 4])\n",
      "tensor([1.5000, 3.0000])\n",
      "torch.Size([2])\n",
      "tensor([[1.5000],\n",
      "        [3.0000]])\n",
      "torch.Size([2, 1])\n",
      "tensor([[-0.5000,  0.5000, -1.5000,  1.5000],\n",
      "        [-2.0000,  1.0000,  0.0000,  1.0000]])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "temp = torch.randint(0,5,(2,4), dtype=torch.float32)\n",
    "print(temp)\n",
    "print(temp.shape)\n",
    "temp2 = temp.mean(dim=1)\n",
    "print(temp2)\n",
    "print(temp2.shape)\n",
    "temp3 = temp.mean(dim=1, keepdim=True)\n",
    "print(temp3)\n",
    "print(temp3.shape)\n",
    "#print(torch.mean(temp, dim=1, keepdim=True)) (just checking syntax variation )\n",
    "#temp4 = temp - temp2#The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 1\n",
    "#print(temp4)\n",
    "#print(temp4.shape)\n",
    "temp5 = temp - temp3\n",
    "print(temp5)\n",
    "print(temp5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d08901e1-63d7-44b0-ac62-f94dec1fdca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., -inf, 0., 0.],\n",
      "        [0., 0., 0., 0., 0., -inf, 0., 0.],\n",
      "        [0., 0., 0., 0., 0., -inf, 0., 0.],\n",
      "        [0., 0., 0., 0., 0., -inf, 0., 0.],\n",
      "        [0., 0., 0., 0., 0., -inf, 0., 0.],\n",
      "        [0., 0., 0., 0., 0., -inf, 0., 0.],\n",
      "        [0., 0., 0., 0., 0., -inf, 0., 0.],\n",
      "        [0., 0., 0., 0., 0., -inf, 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.zeros(8,8)\n",
    "padded_tokens = 3\n",
    "mask[:, -padded_tokens] = -float('inf')\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e0682a-5e30-4767-b396-392eb8dbddee",
   "metadata": {},
   "source": [
    "Decoder architecture details:<br>\n",
    "MAX_SEQ_LEN_DEC = 11 specifically => YYYY-MM-DD and \\<eos> token<br>\n",
    "d_model = 128 (Same as the encoder)<br>\n",
    "Will create a different vocabulary for decoder as only digits, '-', \\<sos> and \\<eos> (I am posing an easier problem for the decoder to learn from - rather than using our earlier vocabulary containing alphabets, other special symbols, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "78a78874-e224-4a3e-8850-4c9df88f1f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "temp = torch.zeros(11,11)\n",
    "temp2 = temp.masked_fill(torch.triu(torch.ones_like(temp), diagonal=1) == 1, float('-inf'))\n",
    "\n",
    "print(temp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "05de1b15-2484-4b82-a724-1595081b6093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '<eos>', '<sos']\n",
      "{'-': 0, '0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, '<eos>': 11, '<sos': 12}\n",
      "{0: '-', 1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9', 11: '<eos>', 12: '<sos'}\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN_DEC = 11\n",
    "EMBEDDING_DIM = 128 #d_model\n",
    "ATTENTION_HEADS = 2\n",
    "vocab = \"0123456789-\"\n",
    "vocab_list_dec = list(vocab)\n",
    "vocab_list_dec.append('<sos')\n",
    "vocab_list_dec.append('<eos>')\n",
    "vocab_list_dec.sort()\n",
    "print(vocab_list_dec)\n",
    "VOCAB_SIZE_DEC = len(vocab_list_dec)\n",
    "ctoi_dec = {ch:i for i,ch in enumerate(vocab_list_dec)}\n",
    "print(ctoi_dec)\n",
    "itoc_dec = {i:ch for i,ch in enumerate(vocab_list_dec)}\n",
    "print(itoc_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d0b9f-6ce7-499b-a1da-574c148db7f0",
   "metadata": {},
   "source": [
    "Cross Masked Multi Head Attention details : <br>\n",
    "It will receive op_embedding (from masked multi head attention block) - token_embeddings<br>\n",
    "It will receive car_enc (contextually aware representations obtained from encoder block) - car_enc <br>\n",
    "token_embeddings = 11 * 128 <br>\n",
    "car_enc = 32 * 128 <br>\n",
    "W_q = 128 * 64 <br>\n",
    "W_v = 128 * 64 <br>\n",
    "W_k = 128 * 64 <br>\n",
    "Since, I want two attention heads <br>\n",
    "W_qs = (2, 64, 64) - these are two parameter matrices for two attention heads <br>\n",
    "W_ks = (2, 64, 64) - these are two parameter matrices for two attention heads <br>\n",
    "W_vs = (2, 64, 64) - these are two parameter matrices for two attention heads <br>\n",
    "\n",
    "I am going to project my token_embeddings and car_enc from 128 dimensional space to 64 dim space (so that they can be fed to the attention heads) via Linear projection\n",
    "token_embeddings = 11 * 64<br>\n",
    "car_enc = 32 * 64 <br>\n",
    "\n",
    "Qs = W_qs @ token_embeddings.T (2, 64, 11) <br>\n",
    "Ks = W_ks @ car_enc.T (2, 64, 32) <br>\n",
    "Vs = W_vs @ car_enc.T (2, 64, 32) <br>\n",
    "\n",
    "Implementing Attention Equation: <br>\n",
    "Qs^T Ks / sqrt(d_k) = (2, 11, 32) <br>\n",
    "softmax across rows in above matrix of 11 * 32 <br>\n",
    "softmax_op @ Vs.T = (2, 11, 64) <br>\n",
    "concatenate: (11, 128)  (Your final embeddings) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c1742002-7e20-4910-a2f4-2c8a86d16000",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, max_seq_len = MAX_SEQ_LEN, ip_embedding_dim = EMBEDDING_DIM, proj_embedding_dim = EMBEDDING_DIM):\n",
    "        super().__init__()\n",
    "        #self.W_q = torch.randn(max_seq_len, embedding_dim)\n",
    "        #self.W_k = torch.randn(max_seq_len, embedding_dim)\n",
    "        #self.W_v = torch.randn(max_seq_len, embedding_dim)\n",
    "        self.W_q = torch.randn(ip_embedding_dim, proj_embedding_dim)\n",
    "        self.W_k = torch.randn(ip_embedding_dim, proj_embedding_dim)\n",
    "        self.W_v = torch.randn(ip_embedding_dim, proj_embedding_dim)\n",
    "'''\n",
    "\n",
    "\n",
    "class Masked_Multi_Head_Attention(nn.Module):\n",
    "    def __init__(self, heads = ATTENTION_HEADS, ip_embedding_dim = EMBEDDING_DIM, max_seq_len = MAX_SEQ_LEN_DEC):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        print(\"REDUCED DIMENSION: \", int(ip_embedding_dim/self.heads))\n",
    "        self.attention_heads = [Attention(ip_embedding_dim = int(ip_embedding_dim/self.heads), proj_embedding_dim = int(ip_embedding_dim/self.heads)) for i in range(heads)]\n",
    "        self.W_qs = [head.W_q for head in self.attention_heads]#learnable\n",
    "        self.W_ks = [head.W_k for head in self.attention_heads]#learnable\n",
    "        self.W_vs = [head.W_v for head in self.attention_heads]#learnable\n",
    "        self.ip_embedding_dim = ip_embedding_dim\n",
    "        self.proj_embedding_dim = int(ip_embedding_dim/heads)\n",
    "        self.token_projection = nn.Linear(ip_embedding_dim, self.proj_embedding_dim)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.linear = nn.Linear(ip_embedding_dim, ip_embedding_dim)\n",
    "        #self.mask = torch.zeros(max_seq_len * max_seq_len)#NOT LEARNABLE\n",
    "        #self.mask[:, -num_padded_tokens] = -float('inf')\n",
    "\n",
    "    def forward(self, token_embeddings):\n",
    "        #I was thinking of computing contextually aware repr from each of the attention heads one-by-one and then concatenating them\n",
    "        #This will work and it's implementation is also pretty striaghtforward, but this defeats the purpose of parallelization\n",
    "        #which can be achieved across the attention heads\n",
    "        W_qs_ = torch.cat(tuple(self.W_qs), dim = 0)\n",
    "        W_ks_ = torch.cat(tuple(self.W_ks), dim = 0)\n",
    "        W_vs_ = torch.cat(tuple(self.W_vs), dim = 0)\n",
    "        print(\"EYLLO\")\n",
    "        print(W_qs_.shape)\n",
    "        print(W_ks_.shape)\n",
    "        print(W_vs_.shape)\n",
    "\n",
    "        print(\"EYLLOSLKDJFLS\")\n",
    "\n",
    "        W_qs_ = W_qs_.view(-1, self.proj_embedding_dim, self.proj_embedding_dim)\n",
    "        W_ks_ = W_ks_.view(-1, self.proj_embedding_dim, self.proj_embedding_dim)\n",
    "        W_vs_ = W_vs_.view(-1, self.proj_embedding_dim, self.proj_embedding_dim)\n",
    "\n",
    "        print(W_qs_.shape)\n",
    "        print(W_ks_.shape)\n",
    "        print(W_vs_.shape)\n",
    "\n",
    "        print(token_embeddings.shape)\n",
    "\n",
    "        token_embeddings_proj = self.token_projection(token_embeddings)\n",
    "\n",
    "        print(token_embeddings_proj.shape)\n",
    "\n",
    "        print(\"W_qs_ shape: \",W_qs_.shape)\n",
    "        print(\"token_embeddings_proj shape: \",token_embeddings_proj.shape)\n",
    "        print(\"token_embeddings_proj transpose shape: \",token_embeddings_proj.T.shape)\n",
    "\n",
    "        #Qs = W_qs_ @ token_embeddings_proj.T\n",
    "        #print(Qs.shape)\n",
    "        #Ks = W_ks_ @ token_embeddings_proj.T\n",
    "        #print(Ks.shape)\n",
    "        squeezed = token_embeddings_proj.T.unsqueeze(0)\n",
    "        print(\"token_embeddings_proj.unsqueeze: \", squeezed.shape )\n",
    "        Qs = torch.matmul(W_qs_, squeezed)\n",
    "        print(Qs.shape)\n",
    "        Ks = torch.matmul(W_ks_, squeezed)\n",
    "        print(Ks.shape)\n",
    "        Vs = torch.matmul(W_vs_, squeezed)\n",
    "        print(Vs.shape)\n",
    "\n",
    "        Qs_Ks = torch.bmm(Qs.transpose(1,2), Ks)/math.sqrt(Ks.shape[1])\n",
    "\n",
    "        print(Qs_Ks.shape)\n",
    "\n",
    "        mask = torch.zeros(self.max_seq_len, self.max_seq_len)\n",
    "        #softmax_op = nn.functional.softmax(Qs_Ks, dim = 1)\n",
    "        #mask[:, -num_padded_tokens:] = -float('inf')\n",
    "        \n",
    "        #mask[-num_padded_tokens:,:] = -float('inf') Padding mask for encoder\n",
    "        mask = mask.masked_fill(torch.triu(torch.ones_like(mask), diagonal=1) == 1, float('-inf'))\n",
    "        \n",
    "        print('MASK', mask)\n",
    "        print(\"MASK SHAPE: \",mask.shape)\n",
    "        softmax_op = nn.functional.softmax(Qs_Ks + mask, dim = 1) #With mask added\n",
    "        print('SOFTMAX OUTPUT: ',softmax_op)\n",
    "\n",
    "        print(softmax_op.shape)\n",
    "\n",
    "        output = torch.bmm(softmax_op, Vs.transpose(1,2))\n",
    "\n",
    "        print(output.shape)\n",
    "\n",
    "        head_outputs = [head_op for head_op in output]\n",
    "\n",
    "        final_output = torch.cat(tuple(head_outputs), dim = 1)\n",
    "\n",
    "        print(final_output.shape)\n",
    "\n",
    "        #YOU CONCATENATED THE OUTPUS OF EACH HEAD, BUT DIDN'T APPLY A LINEAR LAYER AFTERWORDS, directly went to Add Norm Layer\n",
    "        final_output = self.linear(final_output)\n",
    "        #return final_output\n",
    "        return final_output + token_embeddings #Add Layer\n",
    "\n",
    "        #Qs = W_qs @ token_embeddings\n",
    "\n",
    "class Cross_Masked_MultiHead_Attention(nn.Module):\n",
    "    def __init__(self, heads = ATTENTION_HEADS, ip_embedding_dim =  EMBEDDING_DIM, max_seq_len = MAX_SEQ_LEN_DEC):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.attention_heads = [Attention(ip_embedding_dim = int(ip_embedding_dim/heads), proj_embedding_dim = int(ip_embedding_dim/heads)) for i in range(heads)]\n",
    "        self.W_qs = [head.W_q for head in self.attention_heads]\n",
    "        self.W_ks = [head.W_k for head in self.attention_heads]#learnable\n",
    "        self.W_vs = [head.W_v for head in self.attention_heads]#learnable\n",
    "        self.ip_embedding_dim = ip_embedding_dim\n",
    "        self.proj_embedding_dim = int(ip_embedding_dim/heads)\n",
    "        self.token_projection = nn.Linear(ip_embedding_dim, self.proj_embedding_dim)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.linear = nn.Linear(ip_embedding_dim, ip_embedding_dim)\n",
    "        #self.mask = torch.zeros(max_seq_len * max_seq_len)#NOT LEARNABLE\n",
    "        #self.mask[:, -num_padded_tokens] = -float('inf')        \n",
    "        \n",
    "\n",
    "    def forward(self, car_enc, token_embeddings):#contextually aware representations obtained from encoder and op token embeddings\n",
    "        if token_embeddings is None:#Inference wala code yaha pe ayega\n",
    "            return torch.tensor([0] * self.max_seq_len)\n",
    "\n",
    "        print(\"HUHAHAHA\",len(self.W_qs), self.W_qs[0].shape)\n",
    "\n",
    "        W_qs_ = torch.cat(tuple(self.W_qs), dim = 0) #(ip_embedding_dim, proj_embedding_dim)\n",
    "        W_ks_ = torch.cat(tuple(self.W_ks), dim = 0) #(ip_embedding_dim, proj_embedding_dim)\n",
    "        W_vs_ = torch.cat(tuple(self.W_vs), dim = 0) #(ip_embedding_dim, proj_embedding_dim)\n",
    "        print(\"EYLLO\")\n",
    "        print(W_qs_.shape)\n",
    "        print(W_ks_.shape)\n",
    "        print(W_vs_.shape)\n",
    "\n",
    "        print(\"EYLLOSLKDJFLS\")\n",
    "\n",
    "        W_qs_ = W_qs_.view(-1, self.proj_embedding_dim, self.proj_embedding_dim) #(#heads, proj_embedding_dim, proj_embedding_dim)\n",
    "        W_ks_ = W_ks_.view(-1, self.proj_embedding_dim, self.proj_embedding_dim) #(#heads, proj_embedding_dim, proj_embedding_dim)\n",
    "        W_vs_ = W_vs_.view(-1, self.proj_embedding_dim, self.proj_embedding_dim) #(#heads, proj_embedding_dim, proj_embedding_dim)\n",
    "\n",
    "        print(W_qs_.shape)\n",
    "        print(W_ks_.shape)\n",
    "        print(W_vs_.shape)\n",
    "\n",
    "        print(token_embeddings.shape)\n",
    "\n",
    "        token_embeddings_proj = self.token_projection(token_embeddings)\n",
    "\n",
    "        print(token_embeddings_proj.shape)\n",
    "\n",
    "        print(car_enc.shape)\n",
    "\n",
    "        car_enc_proj = self.token_projection(car_enc)\n",
    "\n",
    "        print(car_enc_proj.shape)\n",
    "\n",
    "        unsqueezed_token_embeddings_proj = token_embeddings_proj.T.unsqueeze(0)\n",
    "        print(\"token_embeddings_proj.unsqueeze: \", unsqueezed_token_embeddings_proj.shape )\n",
    "        Qs = torch.matmul(W_qs_, unsqueezed_token_embeddings_proj)\n",
    "        print(Qs.shape)\n",
    "\n",
    "        unsqueezed_car_enc_proj = car_enc_proj.T.unsqueeze(0)\n",
    "\n",
    "        print(\"car_enc_proj.unsqueeze: \", unsqueezed_car_enc_proj.shape)\n",
    "        print(\"W_ks_: \",W_ks_)\n",
    "        print(\"unsqueezed_car_enc_proj: \",unsqueezed_car_enc_proj)\n",
    "        Ks = torch.matmul(W_ks_, unsqueezed_car_enc_proj)\n",
    "        print(Ks.shape)\n",
    "\n",
    "        print(\"Qs: \",Qs)\n",
    "        print(\"Ks: \",Ks)\n",
    "\n",
    "        Vs = torch.matmul(W_vs_, unsqueezed_car_enc_proj)\n",
    "        print(Vs.shape)\n",
    "\n",
    "        #Attention equation implementation starts\n",
    "        Qs_Ks = torch.bmm(Qs.transpose(1,2), Ks)/math.sqrt(Ks.shape[1])\n",
    "\n",
    "        print(\"QS_KS: \",Qs_Ks)\n",
    "\n",
    "        print(Qs_Ks.shape)\n",
    "\n",
    "        #There is no mask needed at cross multi head attention\n",
    "\n",
    "        '''\n",
    "        mask = torch.zeros(token_embeddings.shape[0], car_enc.shape[0])\n",
    "\n",
    "        print(mask.shape)\n",
    "\n",
    "        mask = mask.masked_fill(torch.triu(torch.ones_like(mask), diagonal=1) == 1, float('-inf'))\n",
    "\n",
    "        print(mask)\n",
    "        '''\n",
    "\n",
    "        softmax_op = nn.functional.softmax(Qs_Ks, dim = 1)\n",
    "        print(softmax_op.shape)\n",
    "\n",
    "        output = torch.bmm(softmax_op, Vs.transpose(1,2))\n",
    "\n",
    "        print(output.shape)\n",
    "\n",
    "        head_outputs = [head_op for head_op in output]\n",
    "\n",
    "        final_output = torch.cat(tuple(head_outputs), dim = 1)\n",
    "\n",
    "        print(final_output.shape)\n",
    "        #INSERT A LINEAR LAYER(with dimensions = (ip_embedding, ip_embedding_dim)) HERE ON final_output, so that the concatenated learnt representations are nicely mixed\n",
    "\n",
    "        final_output = self.linear(final_output)\n",
    "\n",
    "        #print(final_output.shape)\n",
    "    \n",
    "        return final_output + token_embeddings\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, max_seq_len = MAX_SEQ_LEN_DEC, vocab_size = VOCAB_SIZE_DEC, d_model = EMBEDDING_DIM):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, d_model)\n",
    "        self.masked_multihead = Masked_Multi_Head_Attention()\n",
    "        self.layer_norm = LayerNorm(d_model)\n",
    "        self.cross_masked_multihead = Cross_Masked_MultiHead_Attention()\n",
    "        self.ffnn = FFNN(d_model)\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, car_enc, tokens=None):\n",
    "        #token_embeddings = self.embedding_layer(tokens)\n",
    "        #token_embeddings = self.embedding_layer(torch.tensor(tokens))\n",
    "        if tokens is None:#Inference vala code ayega yaha pe\n",
    "            return torch.tensor([0] * self.max_seq_len)\n",
    "        token_embeddings = self.embedding_layer(torch.tensor(tokens))\n",
    "        print(token_embeddings)\n",
    "        print(token_embeddings.shape)\n",
    "        car_1 = self.masked_multihead(token_embeddings)\n",
    "        print(car_1)\n",
    "        print(car_1.shape)\n",
    "        car_1_norm = self.layer_norm(car_1)\n",
    "        print(car_1_norm)\n",
    "        print(car_1_norm.shape)\n",
    "\n",
    "        cross_attended_car = self.cross_masked_multihead(car_enc, car_1_norm)\n",
    "        print(cross_attended_car.shape)\n",
    "\n",
    "        cross_attended_car_norm = self.layer_norm(cross_attended_car)\n",
    "        print(cross_attended_car_norm.shape)\n",
    "\n",
    "        car_after_ffnn = self.ffnn(cross_attended_car_norm)\n",
    "        print(car_after_ffnn.shape)\n",
    "\n",
    "        car_after_ffnn_layer_norm = self.layer_norm(car_after_ffnn)\n",
    "        print(car_after_ffnn_layer_norm.shape)\n",
    "\n",
    "        logits = self.linear(car_after_ffnn_layer_norm)\n",
    "        print(\"LOGITS: \",logits.shape)\n",
    "\n",
    "        softmax_op = nn.functional.softmax(logits, dim=1)\n",
    "        print(softmax_op.shape)\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, ip_tokens, op_tokens=None, num_padded_tokens_in_ip=0):#op_tokens will be None during inference and it will contain op_tokens during training (for teacher forcing during initial stages)\n",
    "        car_enc = self.encoder(tokens = ip_tokens, padded_tokens = num_padded_tokens_in_ip)\n",
    "        print(\"car_enc type: \",type(car_enc))\n",
    "        print(\"car_enc: \",car_enc)\n",
    "        print(\"car_enc shape: \",car_enc.shape)\n",
    "        final_op_tokens = self.decoder(car_enc, op_tokens)#should contain token indices, particularly 11 token indices\n",
    "        return final_op_tokens\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "50b05072-074d-4c19-ab06-05ab205c4a83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REDUCED DIMENSION:  64\n",
      "torch.Size([32, 128])\n",
      "tensor([[-0.0069,  1.7548, -1.1532,  ...,  1.2560,  0.5332, -2.1933],\n",
      "        [-1.5602, -0.7389, -0.3998,  ..., -0.2399,  0.2198,  1.0772],\n",
      "        [ 0.5438, -0.3836, -0.6168,  ...,  3.2215,  0.8302, -1.3361],\n",
      "        ...,\n",
      "        [-0.0069,  1.7548, -1.1532,  ...,  1.2560,  0.5332, -2.1933],\n",
      "        [-0.2212, -0.5800, -0.2191,  ..., -0.0268,  0.7745,  0.8087],\n",
      "        [-0.7779, -0.6317,  0.9720,  ...,  0.3528,  1.0134, -0.5293]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([11, 128])\n",
      "EYLLO\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n",
      "EYLLOSLKDJFLS\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([11, 128])\n",
      "torch.Size([11, 64])\n",
      "W_qs_ shape:  torch.Size([2, 64, 64])\n",
      "token_embeddings_proj shape:  torch.Size([11, 64])\n",
      "token_embeddings_proj transpose shape:  torch.Size([64, 11])\n",
      "token_embeddings_proj.unsqueeze:  torch.Size([1, 64, 11])\n",
      "torch.Size([2, 64, 11])\n",
      "torch.Size([2, 64, 11])\n",
      "torch.Size([2, 64, 11])\n",
      "torch.Size([2, 11, 11])\n",
      "MASK tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "MASK SHAPE:  torch.Size([11, 11])\n",
      "SOFTMAX OUTPUT:  tensor([[[5.8032e-14, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [3.6552e-18, 9.8994e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [3.0155e-20, 2.3774e-17, 1.4736e-25, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [9.7829e-01, 5.4457e-05, 4.2992e-08, 1.8032e-07, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [1.9497e-07, 6.9722e-07, 8.4850e-21, 1.3380e-01, 9.1856e-15,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [1.9370e-06, 5.1914e-08, 4.5884e-18, 1.9704e-01, 9.7555e-04,\n",
      "          5.8383e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [1.0855e-02, 3.7811e-09, 5.0000e-01, 3.6484e-14, 3.9242e-36,\n",
      "          3.5184e-11, 3.1775e-13, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [1.9497e-07, 6.9722e-07, 8.4850e-21, 1.3380e-01, 9.1856e-15,\n",
      "          2.5306e-01, 9.9998e-01, 9.1945e-15, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [5.8032e-14, 1.7368e-09, 6.5006e-07, 4.7277e-01, 1.3498e-21,\n",
      "          1.6307e-01, 4.4906e-11, 1.3511e-21, 5.3457e-12, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [3.7081e-07, 1.0006e-02, 1.0198e-07, 6.2591e-02, 9.9902e-01,\n",
      "          4.3145e-05, 2.0886e-05, 1.0000e+00, 3.4158e-05, 4.1514e-17,\n",
      "          0.0000e+00],\n",
      "         [1.0855e-02, 3.7811e-09, 5.0000e-01, 3.6484e-14, 3.9242e-36,\n",
      "          3.5184e-11, 3.1775e-13, 3.9281e-36, 9.9997e-01, 1.0000e+00,\n",
      "          1.0000e+00]],\n",
      "\n",
      "        [[2.6068e-15, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [1.8791e-24, 1.9089e-28, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [4.1393e-23, 9.9997e-01, 2.5414e-30, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [6.1547e-22, 1.1790e-27, 9.5844e-01, 3.2859e-13, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [5.0000e-01, 1.5119e-31, 2.6650e-16, 1.2660e-22, 9.0391e-04,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.1646e-31, 3.2425e-05, 4.1563e-02, 1.7500e-15, 2.2510e-01,\n",
      "          4.1267e-35, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [7.1372e-07, 2.1530e-21, 1.3740e-10, 7.3374e-04, 3.8280e-01,\n",
      "          2.8047e-19, 5.0000e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [5.0000e-01, 1.5119e-31, 2.6650e-16, 1.2660e-22, 9.0391e-04,\n",
      "          3.3087e-18, 5.0742e-22, 2.3106e-03, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.6068e-15, 4.1281e-14, 6.9538e-10, 1.1367e-05, 7.4905e-03,\n",
      "          2.0269e-13, 1.7779e-09, 1.9148e-02, 3.6524e-09, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [3.8639e-23, 1.6870e-26, 7.1670e-17, 9.9852e-01, 4.6920e-13,\n",
      "          1.0000e+00, 6.6885e-30, 1.1994e-12, 5.4138e-17, 1.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [7.1372e-07, 2.1530e-21, 1.3740e-10, 7.3374e-04, 3.8280e-01,\n",
      "          2.8047e-19, 5.0000e-01, 9.7854e-01, 1.0000e+00, 5.3718e-13,\n",
      "          1.0000e+00]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([2, 11, 11])\n",
      "torch.Size([2, 11, 64])\n",
      "torch.Size([11, 128])\n",
      "tensor([[ 3.8625e-02,  1.7099e+00, -1.2229e+00,  ...,  1.3219e+00,\n",
      "          5.2634e-01, -2.2379e+00],\n",
      "        [-3.3396e+00,  5.7711e-01,  3.4391e-01,  ...,  1.8986e-01,\n",
      "         -6.4159e-01,  5.5635e-01],\n",
      "        [ 6.3918e+00, -7.4998e-01,  2.2798e-01,  ...,  7.1709e-01,\n",
      "         -3.1289e-01, -3.5569e+00],\n",
      "        ...,\n",
      "        [-5.9988e-01,  2.4308e+00, -8.7141e-01,  ...,  8.3824e-01,\n",
      "         -1.6347e+00, -3.6782e+00],\n",
      "        [ 8.1736e+00, -8.1227e+00,  2.5071e+00,  ...,  4.0267e+00,\n",
      "          2.0981e+00,  1.5777e+00],\n",
      "        [-6.8820e+00,  5.4872e+00,  8.3129e-01,  ...,  2.5077e-03,\n",
      "          2.7366e+00,  1.1259e+01]], grad_fn=<AddBackward0>)\n",
      "torch.Size([11, 128])\n",
      "tensor([[ -8.2338,  11.3134, -11.3133,  ...,  11.3133,  11.3105, -11.3136],\n",
      "        [-11.3136,  11.3125,  11.3114,  ...,  11.3092, -11.3098,  11.3125],\n",
      "        [ 11.3137, -11.3125,  11.2968,  ...,  11.3122, -11.3072, -11.3137],\n",
      "        ...,\n",
      "        [-11.3123,  11.3136, -11.3130,  ...,  11.3123, -11.3135, -11.3137],\n",
      "        [ 11.3137, -11.3137,  11.3136,  ...,  11.3137,  11.3135,  11.3132],\n",
      "        [-11.3137,  11.3137,  11.3021,  ..., -11.3115,  11.3136,  11.3137]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([11, 128])\n",
      "HUHAHAHA 2 torch.Size([64, 64])\n",
      "EYLLO\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n",
      "EYLLOSLKDJFLS\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([11, 128])\n",
      "torch.Size([11, 64])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 64])\n",
      "token_embeddings_proj.unsqueeze:  torch.Size([1, 64, 11])\n",
      "torch.Size([2, 64, 11])\n",
      "car_enc_proj.unsqueeze:  torch.Size([1, 64, 32])\n",
      "W_ks_:  tensor([[[ 0.8767, -0.7881,  0.7926,  ...,  1.3329,  1.6187, -0.3087],\n",
      "         [-0.9838, -0.7812,  1.6034,  ...,  0.4890, -1.1736, -0.5245],\n",
      "         [ 1.0864, -0.0635,  1.9390,  ...,  1.5917,  0.3558, -0.8631],\n",
      "         ...,\n",
      "         [-1.0947, -0.0785,  1.1459,  ...,  0.7971, -0.0034,  0.8748],\n",
      "         [ 2.0391,  0.0792, -1.0526,  ..., -2.0232, -0.0551,  1.4235],\n",
      "         [ 0.3260, -1.5174, -3.0720,  ..., -1.3079, -0.2252,  0.7844]],\n",
      "\n",
      "        [[-1.2671,  1.2566, -0.6229,  ...,  0.1300,  1.3666,  1.0384],\n",
      "         [-0.1045,  0.9802, -1.1695,  ..., -0.5873, -2.0233, -0.7189],\n",
      "         [-0.7195, -0.3413, -0.2116,  ..., -0.9549, -0.3661, -0.9905],\n",
      "         ...,\n",
      "         [-1.5786, -1.2930, -0.0845,  ...,  0.0504,  2.0477,  1.0477],\n",
      "         [ 0.0528, -1.0844,  1.1809,  ...,  0.2362, -0.7533,  1.0500],\n",
      "         [ 0.5095,  0.1464, -0.4246,  ..., -0.4309,  0.6804, -0.5857]]])\n",
      "unsqueezed_car_enc_proj:  tensor([[[-0.4996,  0.9525, -0.0411,  ..., -0.8746,  0.7418,  0.9107],\n",
      "         [-0.1311,  0.2446, -0.4112,  ...,  0.8430,  0.9660, -0.2858],\n",
      "         [ 0.5656,  0.1970, -0.5790,  ..., -0.4791,  0.0765, -0.0909],\n",
      "         ...,\n",
      "         [ 0.6100,  0.5103,  0.4245,  ...,  0.0972,  0.6652, -0.2924],\n",
      "         [-0.5968, -0.2142, -0.3366,  ...,  0.5000, -0.1517,  0.9430],\n",
      "         [-0.2240, -0.1238,  0.9576,  ..., -0.1467,  0.3885, -0.1667]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "torch.Size([2, 64, 32])\n",
      "Qs:  tensor([[[ -59.8453,   -7.2965,  -17.9223,  ...,  -28.4267,  -49.3502,\n",
      "            81.5890],\n",
      "         [ -26.0967,   19.9493,   66.0528,  ...,  -10.9616,   10.5128,\n",
      "            40.8169],\n",
      "         [  91.5789,  -45.1488,   45.9610,  ...,   89.4172,    0.3940,\n",
      "           -13.6729],\n",
      "         ...,\n",
      "         [   2.5980,  -99.3428,  -57.9527,  ...,  -53.4645,  -20.7167,\n",
      "            95.5697],\n",
      "         [  -8.1642,  -64.4102,  -80.3121,  ..., -105.4102,   74.0146,\n",
      "            13.6215],\n",
      "         [ -17.0731,   44.0093,   32.0764,  ...,  -10.7701,   25.2138,\n",
      "           -35.0230]],\n",
      "\n",
      "        [[ -36.7197,  -52.3642,  -26.0556,  ...,  -73.6929,  105.0046,\n",
      "           148.6284],\n",
      "         [  24.6309,  -51.0775,  -35.5505,  ...,  -64.1861,   -0.6748,\n",
      "            21.0274],\n",
      "         [  47.2705,    1.9272,   -8.3717,  ...,   38.9929,  -25.9740,\n",
      "            34.8428],\n",
      "         ...,\n",
      "         [  -3.4975,   26.6677,  -88.7997,  ...,  -22.6662,  -50.4547,\n",
      "            -5.6829],\n",
      "         [   7.2853,  -22.2910,   50.3911,  ...,   40.2606,  -25.4940,\n",
      "             7.9229],\n",
      "         [ -37.4985, -121.0240,  -64.5706,  ..., -102.1700,   79.9536,\n",
      "           -47.6509]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Ks:  tensor([[[ -0.4454,  -0.8792,   1.4144,  ...,   1.1047,  -1.5780,   2.4724],\n",
      "         [ -3.0141,   2.5103,  -4.7622,  ...,   2.7823,  -6.0545,   0.2693],\n",
      "         [  3.0770,   5.3086,  -3.1894,  ...,   7.6136,   3.8274,  -1.9431],\n",
      "         ...,\n",
      "         [  2.0305, -12.6523,  -1.3238,  ...,  -7.8079,   5.1133,  -0.8369],\n",
      "         [ -8.0825,   2.0413,  -4.0169,  ...,   5.7753,   4.8760,   8.4527],\n",
      "         [  1.1506,  -8.7368,   1.8365,  ...,   0.0260,  -3.8339,  -4.2414]],\n",
      "\n",
      "        [[ -3.7601,  -2.4348,   6.4369,  ...,   8.9461,  -7.5752,  -7.2419],\n",
      "         [  1.1117,  -5.3848,   1.2349,  ...,   0.3901,  -6.7934,   0.4461],\n",
      "         [ -4.1815,   2.1688,  -5.9562,  ...,   4.3071,  -3.1986,  -1.4825],\n",
      "         ...,\n",
      "         [ -1.7180,  -7.1292,   5.4504,  ...,   4.3675,  -7.0746,  -1.6840],\n",
      "         [ -0.6326,  -0.7259,  -1.9921,  ...,  -1.5256,   1.6128,  -3.7857],\n",
      "         [ -5.7831,  -1.6978,   3.5208,  ...,   6.5729,  -5.1533,  -1.4733]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([2, 64, 32])\n",
      "QS_KS:  tensor([[[ 9.7495e+01,  1.2385e+01, -3.7766e+01,  8.8352e+00, -2.6514e+02,\n",
      "           1.4354e+02, -4.3515e+02,  4.4601e+02,  1.3625e+02, -4.6788e+00,\n",
      "           1.6488e+02, -1.4874e+02,  7.3133e+01, -4.8349e+02,  2.2963e+02,\n",
      "           6.6417e+01,  6.6066e+01,  1.4559e+02, -9.9839e+01, -3.9597e+01,\n",
      "           5.9664e+01,  1.9447e+01, -4.4161e+00,  1.2330e+02, -6.6455e+01,\n",
      "          -2.6266e+01,  1.8428e+02, -8.9041e+01, -2.6746e+02,  1.5230e+02,\n",
      "           1.9678e+02, -4.0408e+02],\n",
      "         [-3.6917e+02, -1.0713e+02, -6.1177e+01, -2.9599e+02, -3.8917e+01,\n",
      "           4.5152e+01, -1.1678e+02,  3.1478e+01, -1.5779e+02, -1.4074e+02,\n",
      "           3.2465e+02,  1.8413e+02, -9.7195e+01,  1.1443e+02, -2.6977e+02,\n",
      "           9.9774e+01, -6.1648e+01, -5.7084e+01,  1.9678e+02, -1.0470e+02,\n",
      "           1.9139e+02, -2.6416e+02, -3.5223e+02, -6.5855e+01, -2.0269e+02,\n",
      "          -2.7623e+01, -2.0235e+02, -2.5642e+02,  4.1892e+01,  1.0229e+02,\n",
      "          -1.2217e+02, -1.8235e+02],\n",
      "         [ 1.3202e+02, -3.5918e+01,  3.4602e+02,  2.9350e+01,  6.4546e+00,\n",
      "           1.1275e+02,  5.3371e+01, -1.8437e+02,  1.0569e+02,  6.0277e+01,\n",
      "          -2.8791e+02,  2.0712e+02,  2.6634e+01,  2.5244e+01, -1.7299e+02,\n",
      "          -1.3294e+02, -1.2617e+02,  3.3456e+02, -3.6345e+02, -1.8498e+02,\n",
      "           4.2294e+01, -1.2901e+02, -1.9365e+02, -2.7085e+02,  3.3046e+01,\n",
      "           1.9949e+02, -2.9787e+02,  1.1662e+02,  3.9173e+02, -3.1268e+02,\n",
      "          -3.6910e+02, -1.2727e+02],\n",
      "         [-1.3882e+02,  1.2019e+02,  2.5589e+02,  4.3102e+02, -7.7109e+01,\n",
      "           2.2065e+02,  2.3978e+02,  4.0200e+02,  5.3238e+01,  2.1204e+02,\n",
      "          -4.3201e+02,  3.2369e+02,  2.4965e+02,  5.9773e+01, -7.4890e+00,\n",
      "          -2.5796e+01, -9.7390e+01,  4.1029e+02,  1.7645e+02,  6.1765e+01,\n",
      "           1.2633e+02,  1.3744e+02,  3.1267e+02,  4.9973e+02,  4.3434e+02,\n",
      "           5.6043e+02,  2.6147e+02,  9.3595e+01,  4.4576e+02, -4.6042e+02,\n",
      "          -1.6496e+02, -2.4729e+02],\n",
      "         [ 6.2763e+02,  2.0740e+02,  6.6972e+02,  5.5179e+02,  4.8217e+02,\n",
      "           6.9832e+01,  7.1999e+02, -2.9051e+02,  3.3238e+02,  3.3072e+02,\n",
      "          -4.8248e+02,  2.2689e+02, -2.2402e+02, -3.1274e+01, -2.5075e+01,\n",
      "          -4.5146e+02,  1.3151e+02,  2.8908e+02, -2.2182e+02, -4.9691e+02,\n",
      "           1.3182e+02,  7.1370e+02, -9.5620e+01,  5.6691e+02,  7.7023e+02,\n",
      "           4.4798e+02,  1.3966e+02,  2.7127e+02,  5.1530e+02, -1.2539e+03,\n",
      "          -8.2453e+02, -6.6446e+02],\n",
      "         [-1.3147e+02, -4.1595e+02,  1.3953e+02,  2.5603e+02, -1.2331e+02,\n",
      "           3.4127e+01,  4.1090e+02, -9.9001e+01, -6.8619e+02, -3.3621e+02,\n",
      "           3.3888e+01, -1.4213e+02,  3.7523e+01, -3.5408e+02, -2.4274e+02,\n",
      "          -7.9231e+01, -6.3745e+01,  3.4528e+02,  1.4158e+02,  2.7815e+02,\n",
      "           2.2342e+02, -1.3183e+02,  2.9872e+02,  5.9770e+02, -2.2560e+02,\n",
      "           5.2859e+02, -1.6303e+01,  1.4539e+01, -6.5209e+02, -1.7657e+02,\n",
      "           5.8307e+02,  1.5735e+02],\n",
      "         [ 2.1252e+02, -4.1279e+01,  6.7446e+01, -6.3889e+01, -2.1229e+02,\n",
      "          -7.9985e+01,  1.1718e+02, -9.5372e+01, -2.8463e+02,  1.3937e+02,\n",
      "          -1.1114e+02, -7.5293e+01,  9.3193e+01,  4.1508e+02, -4.0313e+02,\n",
      "          -6.4578e+01,  1.6440e+02, -6.7088e+01,  2.8522e+02,  1.4645e+02,\n",
      "          -6.2548e+01,  1.0535e+02,  1.0450e+02,  2.4186e+02,  1.5462e+02,\n",
      "           1.6714e+02,  5.4150e+01,  2.9738e+02,  7.1250e+01,  1.3888e+02,\n",
      "           4.2040e+02,  3.3985e+02],\n",
      "         [ 3.9350e+02,  9.2484e+00,  6.1726e+02,  4.1011e+02, -2.7205e+01,\n",
      "          -1.8237e+02,  3.2499e+02,  1.5821e+02, -1.0616e+02,  1.2948e+00,\n",
      "          -4.6426e+02, -1.9610e+02,  4.9932e+00, -4.3266e+02, -3.0239e+02,\n",
      "          -3.3452e+02,  4.1237e+01,  1.4430e+02,  1.3275e+02, -2.6697e+02,\n",
      "          -8.6999e+01,  4.7765e+02,  2.9109e+02,  6.8358e+02,  3.8988e+02,\n",
      "           4.4271e+02,  1.1928e+02, -1.0452e+02,  2.8459e+02, -4.5918e+02,\n",
      "          -7.4012e+01, -2.9921e+00],\n",
      "         [ 3.7962e+02,  7.4527e+01,  1.2225e+02, -2.9645e+01, -1.5784e+02,\n",
      "          -1.4609e+02, -1.9684e+02,  7.3326e+01, -5.1602e+02, -8.2866e+01,\n",
      "           9.8569e+01,  8.1542e+01,  1.1867e+02, -5.2685e+02, -1.7761e+02,\n",
      "           6.3520e+00,  1.5872e+02,  2.4429e+02,  1.2376e+02, -5.8700e+01,\n",
      "           2.9121e+01, -9.1041e-01,  3.1160e+01,  8.4277e+01, -1.6315e+02,\n",
      "           1.3548e+02,  2.5417e+02,  2.0654e+01, -2.9721e+02,  2.6765e+01,\n",
      "           3.2989e+02,  9.4738e+00],\n",
      "         [ 6.4892e+01,  4.4567e+02, -1.6275e+02,  3.1743e+02, -2.1514e+02,\n",
      "           6.9855e+01,  3.6629e+01,  2.0477e+02,  2.9106e+02,  2.6704e+02,\n",
      "          -2.9983e+01,  4.4572e+02, -2.2988e+01,  2.3076e+02,  1.0728e+01,\n",
      "           5.1242e+01,  7.2684e+01, -4.3328e+02,  2.6271e+02, -3.6886e+02,\n",
      "          -5.7982e+01,  4.6789e+02,  3.8035e+02,  2.0002e+01,  5.1931e+02,\n",
      "          -1.1006e+02,  2.4333e+02, -9.9044e+01,  4.2321e+02,  1.0881e+02,\n",
      "          -2.4525e+02,  6.0199e+01],\n",
      "         [ 3.7010e+02, -1.0050e+02,  2.4274e+02,  3.4321e+02,  2.0061e+02,\n",
      "          -8.7870e+01,  5.3923e+02, -1.9185e+02, -1.5660e+02, -7.4734e+01,\n",
      "          -4.3211e+02, -2.9807e+02,  2.5958e+01,  5.4442e+02, -9.6795e+00,\n",
      "          -3.5799e+02, -2.3881e+02,  2.0978e+02,  7.9774e+01,  1.7229e+02,\n",
      "           1.7051e+02,  2.8883e+02,  3.1360e+02,  5.6474e+02,  2.8948e+02,\n",
      "           5.3446e+02,  1.8475e+02,  4.6074e+02,  3.0394e+02, -5.2471e+02,\n",
      "           2.6713e+02,  4.5664e+02]],\n",
      "\n",
      "        [[-3.3517e+02, -4.3846e+02, -2.1114e+02,  7.1466e+01,  3.4519e+02,\n",
      "           1.6198e+02,  6.4108e+01,  4.4092e+02, -2.8309e+01,  3.1730e+02,\n",
      "          -1.3050e+02, -7.7912e+01,  2.5806e+02, -2.2576e+02, -1.3152e+02,\n",
      "          -3.4253e+02, -1.3326e+02,  1.9581e+02,  6.8416e+01,  4.7000e+02,\n",
      "           4.3822e+02,  3.8123e+02,  2.5194e+01, -3.0296e+01,  2.7708e+02,\n",
      "           5.6603e+02,  1.2266e+02,  6.4013e+01,  9.0747e+01, -5.4142e+02,\n",
      "          -3.0993e+02, -6.7407e+01],\n",
      "         [ 1.1728e+02,  2.5191e+02, -1.1188e+02,  4.1872e+02,  3.7509e+02,\n",
      "           9.3797e+01, -1.1310e+02,  1.5884e+02, -1.6684e+02, -8.3057e+00,\n",
      "          -3.0269e+01,  7.5612e+01,  6.8968e+01, -6.3984e+01, -1.9883e+02,\n",
      "           8.4187e+00,  5.5226e+01,  6.2709e+02,  8.8236e+00,  3.0949e+01,\n",
      "           5.7099e+01, -1.9581e+02,  4.2448e+02, -1.7746e+02,  7.7793e+01,\n",
      "           7.9405e+01, -5.0588e+00,  2.0784e+02, -1.0946e+02, -2.8297e+02,\n",
      "          -1.7336e+02,  1.2274e+02],\n",
      "         [ 2.6097e+02,  2.5812e+02, -2.2755e+02,  1.7743e+02,  3.4014e+02,\n",
      "           6.3850e+01, -4.6122e+02,  2.9884e+02, -1.3055e+02,  3.5441e+02,\n",
      "           6.9169e+01,  1.0353e+02,  7.0917e+01, -6.2134e+01,  2.0106e+01,\n",
      "          -8.3091e+01,  9.5485e+01,  2.8902e+02,  2.7782e+02, -1.0072e+02,\n",
      "          -1.6991e+02, -1.4029e+02,  3.9026e+02, -2.6320e+02,  9.5844e+01,\n",
      "           1.5895e+02,  1.3048e+02,  1.7978e+02, -1.3070e+02,  1.6953e+02,\n",
      "           1.8308e+02,  2.4510e+02],\n",
      "         [ 1.0271e+02,  2.3683e+02,  3.7299e+01,  1.3906e+02,  8.5029e+01,\n",
      "          -8.8734e+01, -2.3053e+02, -1.8833e+02, -1.1301e+02,  1.0397e+01,\n",
      "           2.2355e+02,  2.6282e+02,  2.0689e+02,  3.1598e+02, -1.3269e+01,\n",
      "           1.3052e+02,  3.5933e+00,  1.1864e+02,  6.0680e+02, -1.9229e+02,\n",
      "          -4.4106e+02,  3.5594e+02, -1.4739e+02, -2.8728e+01,  5.2465e+02,\n",
      "           3.7242e+02, -7.6089e+01,  1.3658e+02,  3.2296e+02, -1.0186e+02,\n",
      "           1.7051e+02, -1.0826e+02],\n",
      "         [ 3.0120e+02, -2.6091e+02,  1.8959e+02, -3.4991e+02, -4.3165e+01,\n",
      "          -3.3532e+02, -3.2295e+02, -2.2931e+02, -3.4536e+01, -1.1802e+02,\n",
      "           1.4418e+02,  4.6997e+02, -9.7033e+01,  1.6709e+02,  1.7859e+02,\n",
      "          -2.8610e+02, -1.4325e+01, -1.6911e+02, -1.2799e+02,  1.4959e+02,\n",
      "          -2.4717e+02,  6.2090e+01, -4.0536e+02, -3.0635e+02,  1.9027e+02,\n",
      "          -2.4007e+02,  1.6726e+02,  1.8691e+01,  5.3928e+01,  6.3154e+02,\n",
      "           5.0066e+01,  1.6753e+02],\n",
      "         [-4.9164e+02,  1.9017e+02, -2.6558e+01,  2.6236e+02,  3.3518e+01,\n",
      "           1.9769e+02,  2.5996e+02, -8.2199e+01,  1.6298e+02,  1.9082e+02,\n",
      "           3.5698e+02,  1.1951e+02,  1.0412e+02, -1.5465e+02, -4.1864e+02,\n",
      "          -1.5638e+02,  1.5668e+02,  1.0244e+02,  1.3184e+02, -3.5906e+02,\n",
      "           1.1155e+02, -1.0972e+02, -1.1391e+02,  1.9439e+02, -2.5066e+02,\n",
      "          -9.9635e+01, -3.4681e+02, -4.3469e+02, -2.6963e+02, -1.4959e+02,\n",
      "          -3.7146e+02, -5.6411e+02],\n",
      "         [ 9.6463e+00, -2.2448e+01,  7.0460e+01, -8.0313e+01, -5.9508e+01,\n",
      "           6.7973e+01, -6.2833e+01,  4.2864e+01, -1.4319e+02,  5.4310e+01,\n",
      "           2.1695e+02, -1.2589e+02,  2.9081e+01,  1.6021e+02,  1.1795e+02,\n",
      "           5.4181e+01, -3.3422e+02, -2.0621e+02,  2.9062e+01,  4.1154e+01,\n",
      "           1.3155e+02, -2.2713e+02, -1.9808e+02,  6.6760e+01, -4.3166e+01,\n",
      "           2.7288e+02, -2.7903e+02, -6.7664e+01,  9.1685e+01, -1.5678e+02,\n",
      "           2.8046e+01, -1.5010e+02],\n",
      "         [-3.8713e+02, -3.4072e+02, -1.6637e+02, -2.4220e+02, -4.4347e+01,\n",
      "           1.3605e+02,  1.6911e+02,  1.9708e+02, -3.2040e+02, -1.1485e+02,\n",
      "           1.1922e+02, -1.3984e+01,  3.9659e+02, -4.4902e+02, -5.5379e+01,\n",
      "          -1.7680e+01,  1.5183e+02,  6.5565e+01, -2.9617e+02, -7.0763e+01,\n",
      "          -1.2005e+02,  2.3101e+02, -2.5063e+02,  9.0014e+01, -3.2016e+02,\n",
      "          -5.6780e+01,  5.1786e+01, -2.5941e+02,  5.0846e+01,  3.0943e+01,\n",
      "           1.9391e+02, -1.8082e+02],\n",
      "         [-1.1487e+02,  6.3106e+01, -4.2750e+02,  1.4853e+02,  4.0583e+02,\n",
      "           1.5085e+02,  5.2924e+01,  5.8487e+02,  1.0819e+02,  3.1679e+02,\n",
      "          -1.8513e+01, -4.5165e+02,  1.1373e+02, -2.6047e+02,  3.1079e+01,\n",
      "          -1.8964e+02,  2.7466e+02,  3.5972e+02, -3.9086e+00,  1.3411e+02,\n",
      "           1.6492e+02,  2.8918e+02,  4.2861e+02, -1.4140e+02, -2.9064e+02,\n",
      "          -1.8300e+01, -3.6648e+01, -1.6303e+02, -2.9174e+02, -1.6149e+02,\n",
      "          -6.9684e+01,  1.7996e+02],\n",
      "         [ 1.3235e+02, -3.8913e+01,  2.4610e+02, -4.2984e+02, -1.4892e+02,\n",
      "          -3.6767e+02, -3.0190e+02, -5.1327e+02,  2.6388e+02, -1.8039e+01,\n",
      "          -1.3692e+02,  3.2458e+02, -9.4402e+01,  5.1624e+01,  3.2050e+02,\n",
      "          -1.3288e+02, -9.7904e+01, -2.6025e+02, -3.9114e+02,  4.0658e+02,\n",
      "          -1.9827e+02, -8.3931e+01, -2.7420e+02, -3.7344e+02, -1.3048e+02,\n",
      "          -1.6571e+02, -1.0665e+02, -9.8751e+00,  2.3864e+02,  4.6401e+02,\n",
      "          -2.0081e+02, -2.0298e+01],\n",
      "         [-4.7063e+02, -1.2335e+02, -1.5269e+02,  3.6112e+01, -6.3862e+02,\n",
      "          -1.4499e+02,  1.7879e+02,  1.6689e+01, -3.2338e+01,  3.9518e+01,\n",
      "           5.3940e+02,  2.9882e+02,  4.7133e+01,  2.8383e+02,  1.8537e+02,\n",
      "           4.1532e+01,  1.0268e+02, -9.3523e+02,  2.2966e+02,  1.8961e+02,\n",
      "          -2.8467e+02,  2.9835e+02, -5.6384e+02,  1.2372e+02, -3.8827e+02,\n",
      "          -1.3242e+02, -1.7287e+02, -2.9254e+02, -4.1287e+01,  1.9258e+02,\n",
      "          -1.4969e+02, -2.4110e+02]]], grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 11, 32])\n",
      "torch.Size([2, 11, 32])\n",
      "torch.Size([2, 11, 64])\n",
      "torch.Size([11, 128])\n",
      "torch.Size([11, 128])\n",
      "torch.Size([11, 128])\n",
      "torch.Size([11, 128])\n",
      "torch.Size([11, 128])\n",
      "LOGITS:  torch.Size([11, 13])\n",
      "torch.Size([11, 13])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0.8159,   0.9526,   4.0626,  -1.0285,  -0.1347, -10.2289,  -0.4143,\n",
       "          -1.1155,  11.8756,  -1.2605,   4.3895,  -4.9434,  -1.0471],\n",
       "        [  3.8913,  -4.6237,   2.3252,   3.2112,   0.0979, -10.1650,   1.0539,\n",
       "           2.2344,  -1.3694,  -3.1331,   1.2873,   1.1184,  -3.4961],\n",
       "        [ -6.9505,   8.9966,   1.0496,   9.6139,   4.7723,   0.4509, -11.6586,\n",
       "           4.5179, -12.4108,  -6.4686,   8.3179,   4.9169,  -1.5999],\n",
       "        [  7.5333,  12.1417,   5.9286,  -0.7985,  -0.7742,  -2.7438,   0.0250,\n",
       "           5.4299,  -1.6144,  -7.3389,   3.8814, -11.3878,  -0.6786],\n",
       "        [ -8.2901,   3.3453,  -2.4744,  -9.6144,  -3.8608,  -2.4068,  -7.4242,\n",
       "           5.7768,  -9.1445,  -0.2286,   2.5977, -15.7328,  -1.4880],\n",
       "        [  3.9572,  -2.3089,   3.8112, -15.9238,  -9.5287, -13.7192,  -7.4584,\n",
       "           1.7347,   0.7506,   4.7978,  -3.3110,   3.7245,  -1.4338],\n",
       "        [ -3.3364,  11.3614,  -0.7551,   5.4343,   4.2162,  -4.3309,   1.7045,\n",
       "          13.6422,  -2.9986,  -4.1630,   8.9540,   1.2502, -10.9155],\n",
       "        [  9.7097,  10.0671,   1.6781,   4.3910,  -8.7987,  -0.0231,  -2.3501,\n",
       "           0.8954,  12.3454,  -2.6032,   7.3777,  -6.5601,  -2.1229],\n",
       "        [ -2.9523,   6.1316,  12.0396,   2.2831,  -6.0709, -15.3030,  -1.0594,\n",
       "          -5.1802,  16.0322,   3.7438,   7.7685,   0.4599,  -2.4071],\n",
       "        [ -9.2976,  -8.2878,  -6.9008,  -2.5499,   9.8438,   6.2821,   3.4599,\n",
       "          -4.8245,  -4.4157,   4.3140, -12.6506,   0.8359,  -1.1751],\n",
       "        [ -5.5473,  13.1856,  13.2644,  -0.5805,   7.4674,   6.2693,   1.9024,\n",
       "          12.6100,   5.8151,  -6.9893,  14.5728,  -0.5407,  -4.8621]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder()\n",
    "date_example = \"1947-08-15\"\n",
    "token_ids = []\n",
    "for ch in date_example:\n",
    "    token_ids.append(ctoi[ch])\n",
    "token_ids.append(ctoi_dec[\"<eos>\"])\n",
    "dummy_car_enc_token_ids = torch.randn(32,128)\n",
    "#dummy_car_enc_token_ids = [0] * 32 (wrong since, we need to feed 32 car_enc embeddings)\n",
    "print(dummy_car_enc_token_ids.shape)\n",
    "decoder(dummy_car_enc_token_ids, token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1715114c-8936-4b14-af27-e6c289465e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10th November 2000\n",
      "10th november 2000\n",
      "[4, 3, 34, 24, 0, 29, 30, 36, 21, 28, 18, 21, 32, 0, 5, 3, 3, 3, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
      "14\n",
      "[3, 1, 1, 1, 0, 2, 2, 0, 2, 1, 11]\n"
     ]
    }
   ],
   "source": [
    "sample_ip = \"10th November 2000\"\n",
    "sample_op = \"2000-11-10\"\n",
    "def get_ip_token_indices(str):\n",
    "    tokens = []\n",
    "    print(str)\n",
    "    str = str.lower()\n",
    "    print(str)\n",
    "    for ch in str:\n",
    "        tokens.append(ctoi[ch])\n",
    "    padded_tokens = MAX_SEQ_LEN - len(tokens)\n",
    "    while len(tokens) < MAX_SEQ_LEN:\n",
    "        tokens.append(ctoi[\"<pad>\"])\n",
    "    return tokens, padded_tokens\n",
    "\n",
    "def get_op_token_indices(str):\n",
    "    tokens = []\n",
    "    for ch in str:\n",
    "        tokens.append(ctoi_dec[ch])\n",
    "    tokens.append(ctoi_dec[\"<eos>\"])\n",
    "    return tokens\n",
    "sample_ip_tokens, padded_tokens = get_ip_token_indices(sample_ip)\n",
    "sample_op_tokens = get_op_token_indices(sample_op)\n",
    "print(sample_ip_tokens)\n",
    "print(padded_tokens)\n",
    "print(sample_op_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b48c65ca-1974-460f-85a7-9f18b8761381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REDUCED DIMENSION:  64\n",
      "REDUCED DIMENSION:  64\n",
      "TOKENS TOKENS:  [4, 3, 34, 24, 0, 29, 30, 36, 21, 28, 18, 21, 32, 0, 5, 3, 3, 3, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
      "PADDED TOKENS:  14\n",
      "EYLLO\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n",
      "EYLLOSLKDJFLS\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 64])\n",
      "W_qs_ shape:  torch.Size([2, 64, 64])\n",
      "token_embeddings_proj shape:  torch.Size([32, 64])\n",
      "token_embeddings_proj transpose shape:  torch.Size([64, 32])\n",
      "token_embeddings_proj.unsqueeze:  torch.Size([1, 64, 32])\n",
      "torch.Size([2, 64, 32])\n",
      "torch.Size([2, 64, 32])\n",
      "torch.Size([2, 64, 32])\n",
      "torch.Size([2, 32, 32])\n",
      "MASK tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]])\n",
      "MASK SHAPE:  torch.Size([32, 32])\n",
      "SOFTMAX OUTPUT:  tensor([[[2.2230e-14, 2.0193e-14, 2.4925e-16,  ..., 7.4082e-03,\n",
      "          7.4082e-03, 7.4082e-03],\n",
      "         [2.9691e-26, 2.2409e-01, 1.0580e-15,  ..., 1.2891e-01,\n",
      "          1.2891e-01, 1.2891e-01],\n",
      "         [2.3131e-20, 6.8369e-09, 7.3382e-03,  ..., 6.0965e-02,\n",
      "          6.0965e-02, 6.0965e-02],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[4.7579e-06, 1.8143e-24, 9.2904e-01,  ..., 5.7180e-03,\n",
      "          5.7180e-03, 5.7180e-03],\n",
      "         [3.4391e-13, 9.0594e-27, 1.7976e-14,  ..., 7.6299e-02,\n",
      "          7.6299e-02, 7.6299e-02],\n",
      "         [1.0713e-13, 2.0281e-22, 3.8546e-21,  ..., 2.2365e-01,\n",
      "          2.2365e-01, 2.2365e-01],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([2, 32, 32])\n",
      "torch.Size([2, 32, 64])\n",
      "torch.Size([32, 128])\n",
      "CONTEXTUALLY AWARE REPR:  tensor([[ 4.5547, -1.0806, -0.7818,  ..., -0.1533, -1.4396, -0.9013],\n",
      "        [-2.4278,  2.6749,  2.0670,  ...,  0.6563, -2.7822, -1.5342],\n",
      "        [ 0.8375,  3.3756,  1.5511,  ..., -2.2170, -0.6450,  3.0609],\n",
      "        ...,\n",
      "        [ 0.0628,  0.0477,  0.0790,  ...,  0.0064,  0.0575, -0.0127],\n",
      "        [ 0.0628,  0.0477,  0.0790,  ...,  0.0064,  0.0575, -0.0127],\n",
      "        [ 0.0628,  0.0477,  0.0790,  ...,  0.0064,  0.0575, -0.0127]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "shape:  torch.Size([32, 128])\n",
      "layer normed car shape:  torch.Size([32, 128])\n",
      "layer normed car after ffnn shape:  torch.Size([32, 128])\n",
      "car_enc type:  <class 'torch.Tensor'>\n",
      "car_enc:  tensor([[ 11.3137, -11.3137, -11.3137,  ..., -11.3137, -11.3137, -11.3137],\n",
      "        [-11.3137,  11.3137,  11.3137,  ...,  11.3137, -11.3137, -11.3137],\n",
      "        [ 11.3137,  11.3137,  11.3137,  ..., -11.3137, -11.3137,  11.3137],\n",
      "        ...,\n",
      "        [ 11.3137,  11.3137,  11.3137,  ...,  11.3137,  11.3137, -11.3137],\n",
      "        [ 11.3137,  11.3137,  11.3137,  ...,  11.3137,  11.3137, -11.3137],\n",
      "        [ 11.3137,  11.3137,  11.3137,  ...,  11.3137,  11.3137, -11.3137]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "car_enc shape:  torch.Size([32, 128])\n",
      "tensor([[ 0.0395, -0.0267, -0.4558,  ...,  0.5552, -0.0096,  0.4167],\n",
      "        [ 1.2033,  0.7261,  1.6735,  ..., -0.0024, -0.2485,  0.1541],\n",
      "        [ 1.2033,  0.7261,  1.6735,  ..., -0.0024, -0.2485,  0.1541],\n",
      "        ...,\n",
      "        [ 1.0236, -0.1559, -1.2380,  ..., -0.8086, -0.3962,  0.8860],\n",
      "        [ 1.2033,  0.7261,  1.6735,  ..., -0.0024, -0.2485,  0.1541],\n",
      "        [-1.1499,  0.5331, -0.7046,  ...,  2.3202, -0.3109, -0.7703]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([11, 128])\n",
      "EYLLO\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n",
      "EYLLOSLKDJFLS\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([11, 128])\n",
      "torch.Size([11, 64])\n",
      "W_qs_ shape:  torch.Size([2, 64, 64])\n",
      "token_embeddings_proj shape:  torch.Size([11, 64])\n",
      "token_embeddings_proj transpose shape:  torch.Size([64, 11])\n",
      "token_embeddings_proj.unsqueeze:  torch.Size([1, 64, 11])\n",
      "torch.Size([2, 64, 11])\n",
      "torch.Size([2, 64, 11])\n",
      "torch.Size([2, 64, 11])\n",
      "torch.Size([2, 11, 11])\n",
      "MASK tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "MASK SHAPE:  torch.Size([11, 11])\n",
      "SOFTMAX OUTPUT:  tensor([[[7.9305e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.0507e-14, 2.5000e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.0507e-14, 2.5000e-01, 3.3333e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.0507e-14, 2.5000e-01, 3.3333e-01, 5.0000e-01, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [7.9225e-17, 1.2246e-15, 1.6328e-15, 2.4492e-15, 4.9999e-01,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.9971e-27, 9.0099e-07, 1.2013e-06, 1.8020e-06, 7.6031e-12,\n",
      "          6.1731e-13, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.9971e-27, 9.0099e-07, 1.2013e-06, 1.8020e-06, 7.6031e-12,\n",
      "          6.1731e-13, 6.1731e-13, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [7.9225e-17, 1.2246e-15, 1.6328e-15, 2.4492e-15, 4.9999e-01,\n",
      "          2.6572e-24, 2.6572e-24, 9.9998e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.9971e-27, 9.0099e-07, 1.2013e-06, 1.8020e-06, 7.6031e-12,\n",
      "          6.1731e-13, 6.1731e-13, 1.5206e-11, 6.1731e-13, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.0507e-14, 2.5000e-01, 3.3333e-01, 5.0000e-01, 9.1321e-06,\n",
      "          1.0000e+00, 1.0000e+00, 1.8264e-05, 1.0000e+00, 1.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [1.0000e+00, 1.4786e-35, 1.9715e-35, 2.9572e-35, 1.0952e-06,\n",
      "          1.1787e-21, 1.1787e-21, 2.1904e-06, 1.1787e-21, 5.9144e-35,\n",
      "          1.0000e+00]],\n",
      "\n",
      "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.2813e-20, 3.7013e-13, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.2813e-20, 3.7013e-13, 3.7013e-13, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.2813e-20, 3.7013e-13, 3.7013e-13, 3.7013e-13, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [5.7755e-16, 5.0000e-01, 5.0000e-01, 5.0000e-01, 5.0000e-01,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [4.4277e-22, 2.5816e-09, 2.5816e-09, 2.5816e-09, 2.9679e-09,\n",
      "          3.2845e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [4.4277e-22, 2.5816e-09, 2.5816e-09, 2.5816e-09, 2.9679e-09,\n",
      "          3.2845e-01, 4.8908e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [5.7755e-16, 5.0000e-01, 5.0000e-01, 5.0000e-01, 5.0000e-01,\n",
      "          6.4186e-37, 9.5578e-37, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [4.4277e-22, 2.5816e-09, 2.5816e-09, 2.5816e-09, 2.9679e-09,\n",
      "          3.2845e-01, 4.8908e-01, 5.9358e-09, 9.5726e-01, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.2813e-20, 3.7013e-13, 3.7013e-13, 3.7013e-13, 5.5794e-11,\n",
      "          1.4664e-02, 2.1836e-02, 1.1159e-10, 4.2739e-02, 9.9699e-01,\n",
      "          0.0000e+00],\n",
      "         [2.8987e-12, 1.1174e-15, 1.1174e-15, 1.1174e-15, 4.2461e-27,\n",
      "          4.4842e-11, 6.6773e-11, 8.4921e-27, 1.3069e-10, 3.0098e-03,\n",
      "          1.0000e+00]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([2, 11, 11])\n",
      "torch.Size([2, 11, 64])\n",
      "torch.Size([11, 128])\n",
      "tensor([[ 0.9578,  0.7151, -1.3217,  ...,  0.1712, -1.9366,  1.8272],\n",
      "        [ 1.1382,  1.2425,  1.5393,  ..., -0.6182,  0.0259,  1.0483],\n",
      "        [ 1.0729,  1.8437,  1.4415,  ..., -1.3608,  0.3045,  2.2058],\n",
      "        ...,\n",
      "        [-0.7247,  1.9510, -2.9828,  ..., -1.6862,  0.5481, -0.1492],\n",
      "        [-5.9693,  9.7725,  9.0524,  ..., -8.2029, 10.7404, 16.7904],\n",
      "        [-6.6304, -2.8562, -3.8201,  ...,  5.4127, -1.2898, -5.9626]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([11, 128])\n",
      "tensor([[ 11.3130,  11.3125, -11.3133,  ...,  11.3006, -11.3135,  11.3135],\n",
      "        [ 11.3132,  11.3133,  11.3134,  ..., -11.3117,  10.9068,  11.3131],\n",
      "        [ 11.3131,  11.3135,  11.3134,  ..., -11.3133,  11.3052,  11.3136],\n",
      "        ...,\n",
      "        [-11.3121,  11.3135, -11.3136,  ..., -11.3134,  11.3117, -11.2458],\n",
      "        [-11.3137,  11.3137,  11.3137,  ..., -11.3137,  11.3137,  11.3137],\n",
      "        [-11.3137, -11.3136, -11.3137,  ...,  11.3137, -11.3135, -11.3137]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([11, 128])\n",
      "HUHAHAHA 2 torch.Size([64, 64])\n",
      "EYLLO\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n",
      "EYLLOSLKDJFLS\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([2, 64, 64])\n",
      "torch.Size([11, 128])\n",
      "torch.Size([11, 64])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 64])\n",
      "token_embeddings_proj.unsqueeze:  torch.Size([1, 64, 11])\n",
      "torch.Size([2, 64, 11])\n",
      "car_enc_proj.unsqueeze:  torch.Size([1, 64, 32])\n",
      "W_ks_:  tensor([[[-1.6284,  0.5863, -0.0569,  ..., -0.7757, -0.7374,  1.7534],\n",
      "         [-0.0142,  0.2682, -0.2032,  ...,  1.3375,  0.0095, -1.9221],\n",
      "         [-1.0749, -0.3075,  1.5178,  ...,  0.1429, -0.6640,  1.5459],\n",
      "         ...,\n",
      "         [ 0.1195, -0.7500,  1.2975,  ...,  1.8715,  0.8482,  0.5878],\n",
      "         [ 0.4019, -0.2382, -0.8608,  ..., -1.5756,  0.6200, -0.7402],\n",
      "         [-0.2131, -1.0793,  0.6455,  ..., -0.6357, -0.2458,  0.4305]],\n",
      "\n",
      "        [[-0.0910, -0.1138, -1.4332,  ..., -1.1863,  0.7581,  0.0815],\n",
      "         [ 1.2261, -0.3307, -0.4925,  ..., -0.1638, -0.3615, -1.1749],\n",
      "         [-0.4366,  0.2506,  0.1690,  ...,  0.2540, -1.0984, -0.6726],\n",
      "         ...,\n",
      "         [ 0.1608,  0.1496, -0.4772,  ...,  0.4897, -0.2997,  1.1599],\n",
      "         [ 0.0377, -1.3408,  1.2434,  ..., -1.4874,  0.2506, -1.5970],\n",
      "         [ 0.6060,  0.8856, -0.4237,  ...,  0.8924, -1.0124,  1.1207]]])\n",
      "unsqueezed_car_enc_proj:  tensor([[[ -1.2200,   6.7506,  -7.1882,  ...,   6.2265,   6.2265,   6.2265],\n",
      "         [  6.6160,  -2.2029, -12.9135,  ...,   0.0445,   0.0445,   0.0445],\n",
      "         [ -9.2052,   2.4029,   7.9460,  ...,   4.4353,   4.4353,   4.4353],\n",
      "         ...,\n",
      "         [  5.6162,   8.2801,  12.1447,  ...,  -9.1654,  -9.1654,  -9.1654],\n",
      "         [ -1.3613,  -2.0809,  -3.6975,  ...,  -1.2465,  -1.2465,  -1.2465],\n",
      "         [  1.9739,   7.8420,   3.4100,  ...,  -4.3978,  -4.3978,  -4.3978]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "torch.Size([2, 64, 32])\n",
      "Qs:  tensor([[[ -97.0556,  -10.0838,  -56.9013,  ...,  144.8887, -112.4459,\n",
      "           -40.5951],\n",
      "         [  45.4142,  -58.9682, -119.0381,  ...,  -26.9922,   -2.2749,\n",
      "           -41.6335],\n",
      "         [   4.9228,  119.1138,  101.8446,  ...,   59.7830,   75.1289,\n",
      "           -71.3315],\n",
      "         ...,\n",
      "         [ -95.8254,  -22.6040,   14.8410,  ...,   -5.4229,   -2.9292,\n",
      "           -65.1347],\n",
      "         [  17.4447,   18.0591,   19.7275,  ...,  -14.4986,  -25.4114,\n",
      "             3.2885],\n",
      "         [  -7.2555,  -61.6417,  -65.4643,  ...,  -28.9351,   48.7311,\n",
      "            -0.5673]],\n",
      "\n",
      "        [[ -41.6823,   16.8173,    7.8903,  ...,   58.6381,  -35.1887,\n",
      "           -54.2463],\n",
      "         [ -47.0476,   -4.8952,    6.0673,  ...,   78.5687,    1.6413,\n",
      "            -0.4220],\n",
      "         [ -77.8601,  -83.0479,  -43.0845,  ...,   76.1436,  -43.1514,\n",
      "            59.2230],\n",
      "         ...,\n",
      "         [-102.2751,    1.3614,  -37.4312,  ...,  182.2062,   41.3989,\n",
      "           -48.5847],\n",
      "         [  -7.5376,   53.8998,  -13.1236,  ...,  125.3261,  -30.9163,\n",
      "           -61.1103],\n",
      "         [   6.4225,   37.4730,   -2.3690,  ...,    9.2214,  -18.0692,\n",
      "           -15.3108]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Ks:  tensor([[[  72.7171,   15.0092,   32.2507,  ...,   39.3911,   39.3911,\n",
      "            39.3911],\n",
      "         [  16.7827,   11.1617,   93.9303,  ...,   95.0643,   95.0643,\n",
      "            95.0643],\n",
      "         [ -48.1417,  -14.6501,  -17.4606,  ...,  -69.3749,  -69.3749,\n",
      "           -69.3749],\n",
      "         ...,\n",
      "         [ -43.1650,   60.2587,   42.5094,  ..., -144.6620, -144.6619,\n",
      "          -144.6619],\n",
      "         [ -70.8517,   29.4424,   26.9139,  ...,   58.5770,   58.5770,\n",
      "            58.5770],\n",
      "         [ -73.5397,  -61.9040,  103.2684,  ...,    5.2280,    5.2280,\n",
      "             5.2280]],\n",
      "\n",
      "        [[  43.4332,  -21.9451,  -51.2205,  ...,   -5.8086,   -5.8087,\n",
      "            -5.8087],\n",
      "         [   6.5579,  -24.3761,   48.2363,  ...,   29.5130,   29.5130,\n",
      "            29.5130],\n",
      "         [  20.5414,  -23.2475,  -34.8199,  ...,  -25.6237,  -25.6237,\n",
      "           -25.6237],\n",
      "         ...,\n",
      "         [  39.1411,   42.7585,   90.5033,  ...,  107.4001,  107.4002,\n",
      "           107.4002],\n",
      "         [-134.4351,   22.6211,  -94.3892,  ...,   38.5215,   38.5215,\n",
      "            38.5215],\n",
      "         [  61.3368,   68.5339,  -78.7213,  ...,  -16.5477,  -16.5477,\n",
      "           -16.5477]]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([2, 64, 32])\n",
      "QS_KS:  tensor([[[ -594.9065, -2801.1504,  -269.2443,   401.2444,  2889.4561,\n",
      "          -5153.7280,   105.9066, -4983.4297, -4566.8657,  1511.1577,\n",
      "          -2545.6238, -4566.8657, -1998.3842,  2889.4561, -1299.1423,\n",
      "          -2801.1506, -2801.1504, -2801.1504,  2883.5635,  2883.5635,\n",
      "           2883.5635,  2883.5635,  2883.5637,  2883.5637,  2883.5635,\n",
      "           2883.5635,  2883.5635,  2883.5635,  2883.5635,  2883.5635,\n",
      "           2883.5635,  2883.5635],\n",
      "         [-2293.8501, -4533.8257, -5674.5166,  1909.1429,   458.5491,\n",
      "           -739.4776,  3536.0513,  2280.0298,  1277.5610, -1518.8218,\n",
      "          -2997.0879,  1277.5610, -1801.7819,   458.5491, -1922.1780,\n",
      "          -4533.8257, -4533.8257, -4533.8257,  3933.7151,  3933.7151,\n",
      "           3933.7151,  3933.7151,  3933.7139,  3933.7139,  3933.7151,\n",
      "           3933.7151,  3933.7151,  3933.7151,  3933.7151,  3933.7151,\n",
      "           3933.7131,  3933.7131],\n",
      "         [-1574.0250, -3924.3892, -5231.8647,   824.0131, -2753.9949,\n",
      "           -233.2987,  2003.7880,  2343.6301,   307.2653, -1700.1322,\n",
      "          -2430.9087,   307.2653, -4059.9136, -2753.9949, -2732.9736,\n",
      "          -3924.3892, -3924.3892, -3924.3892,  -676.8996,  -676.8996,\n",
      "           -676.8996,  -676.8996,  -676.9011,  -676.9011,  -676.8996,\n",
      "           -676.8996,  -676.8996,  -676.8996,  -676.8996,  -676.8996,\n",
      "           -676.9003,  -676.9003],\n",
      "         [ -850.8478, -3310.0535, -2904.6121,   254.6830, -2522.3542,\n",
      "           -430.6498,  1730.7585,   658.9144,  1397.4424, -1035.5730,\n",
      "          -1239.2612,  1397.4424, -2879.6326, -2522.3542, -1792.8868,\n",
      "          -3310.0530, -3310.0535, -3310.0535, -1484.7533, -1484.7533,\n",
      "          -1484.7533, -1484.7533, -1484.7542, -1484.7542, -1484.7533,\n",
      "          -1484.7533, -1484.7533, -1484.7533, -1484.7533, -1484.7533,\n",
      "          -1484.7537, -1484.7537],\n",
      "         [  635.5590, -1143.2920,    12.1436,  -881.1539, -3103.1641,\n",
      "           3659.3684,   -72.4834,   667.0369,  4980.9058, -1615.0571,\n",
      "           -371.5024,  4980.9058,   323.0744, -3103.1641,  3393.1836,\n",
      "          -1143.2916, -1143.2920, -1143.2920,  2007.3435,  2007.3435,\n",
      "           2007.3435,  2007.3435,  2007.3424,  2007.3424,  2007.3435,\n",
      "           2007.3435,  2007.3435,  2007.3435,  2007.3435,  2007.3435,\n",
      "           2007.3425,  2007.3425],\n",
      "         [ 3663.6750,   451.9056, -2101.9561, -2226.1433, -1401.7625,\n",
      "          -6129.9585,  1175.1946, -3504.0627, -3492.0359, -2035.3650,\n",
      "          -1625.6744, -3492.0359, -7676.0469, -1401.7625, -7500.6240,\n",
      "            451.9056,   451.9056,   451.9056,   472.5131,   472.5131,\n",
      "            472.5131,   472.5131,   472.5138,   472.5138,   472.5131,\n",
      "            472.5131,   472.5131,   472.5131,   472.5131,   472.5131,\n",
      "            472.5131,   472.5131],\n",
      "         [ 3707.9880,  1628.8866, -3384.4177,   -51.8507,  -502.2654,\n",
      "          -5536.8555,  2472.1277,  -807.0679, -4073.4895,  -635.5750,\n",
      "           -868.2089, -4073.4895, -4554.5645,  -502.2654, -7662.5186,\n",
      "           1628.8871,  1628.8866,  1628.8866, -1021.9103, -1021.9103,\n",
      "          -1021.9103, -1021.9103, -1021.9096, -1021.9096, -1021.9103,\n",
      "          -1021.9103, -1021.9103, -1021.9103, -1021.9103, -1021.9103,\n",
      "          -1021.9100, -1021.9100],\n",
      "         [-1624.5266,  2174.0046,  -476.1270,  3641.3760, -3540.5156,\n",
      "           4432.3560, -2794.4377, -1245.0422,  1473.8086, -1071.7747,\n",
      "           3888.6777,  1473.8086,  1556.6219, -3540.5156,   496.2336,\n",
      "           2174.0039,  2174.0046,  2174.0046,  -980.7996,  -980.7996,\n",
      "           -980.7996,  -980.7996,  -980.7990,  -980.7990,  -980.7996,\n",
      "           -980.7996,  -980.7996,  -980.7996,  -980.7996,  -980.7996,\n",
      "           -980.7986,  -980.7986],\n",
      "         [ 3281.8975,   512.5761, -2108.1765,  -711.5544, -1719.8583,\n",
      "          -3449.7168,  2556.7024,   996.9459, -2947.7388,  -225.5246,\n",
      "           -534.1840, -2947.7388, -1919.7454, -1719.8583, -5175.2178,\n",
      "            512.5770,   512.5761,   512.5761, -1702.0775, -1702.0775,\n",
      "          -1702.0775, -1702.0775, -1702.0770, -1702.0770, -1702.0775,\n",
      "          -1702.0775, -1702.0775, -1702.0775, -1702.0775, -1702.0775,\n",
      "          -1702.0776, -1702.0776],\n",
      "         [  543.2289, -2875.3020,  2287.4937, -3787.2495, -5733.9307,\n",
      "           4462.5010,  2994.2708,  3460.5137,  1677.0493,   258.5136,\n",
      "           2013.6688,  1677.0493,  -901.1111, -5733.9307,  2523.5930,\n",
      "          -2875.3008, -2875.3020, -2875.3020,   772.8942,   772.8942,\n",
      "            772.8942,   772.8942,   772.8935,   772.8935,   772.8942,\n",
      "            772.8942,   772.8942,   772.8942,   772.8942,   772.8942,\n",
      "            772.8934,   772.8934],\n",
      "         [ 3312.3191, -2854.0054,   373.8574, -1431.9146,  -653.0875,\n",
      "            640.8113, -1926.2366,  -372.7366, -2223.5525,  2800.7339,\n",
      "          -1118.1904, -2223.5525,   431.6158,  -653.0875,  -718.8362,\n",
      "          -2854.0039, -2854.0054, -2854.0054,  1397.7616,  1397.7616,\n",
      "           1397.7616,  1397.7616,  1397.7618,  1397.7618,  1397.7616,\n",
      "           1397.7616,  1397.7616,  1397.7616,  1397.7616,  1397.7616,\n",
      "           1397.7618,  1397.7618]],\n",
      "\n",
      "        [[ -351.4058,  1253.6615, -1694.9484,  -112.4777,  2504.5659,\n",
      "           2478.9939, -2342.2698,  1768.3811,  1317.9122,   664.3130,\n",
      "           2270.9690,  1317.9122,  -907.1064,  2504.5659, -2142.4744,\n",
      "           1253.6620,  1253.6615,  1253.6615,  2648.6663,  2648.6663,\n",
      "           2648.6663,  2648.6663,  2648.6658,  2648.6658,  2648.6663,\n",
      "           2648.6663,  2648.6663,  2648.6663,  2648.6663,  2648.6663,\n",
      "           2648.6675,  2648.6675],\n",
      "         [  196.1872,  3805.5173,  2183.7974,   201.6783,  5813.1914,\n",
      "           5065.9316, -1814.8652,  -504.7657, -1539.2513,  -912.2681,\n",
      "          -4637.5449, -1539.2513, -4333.8301,  5813.1914, -1970.9034,\n",
      "           3805.5166,  3805.5173,  3805.5173,  2984.3457,  2984.3457,\n",
      "           2984.3457,  2984.3457,  2984.3450,  2984.3450,  2984.3457,\n",
      "           2984.3457,  2984.3457,  2984.3457,  2984.3457,  2984.3457,\n",
      "           2984.3464,  2984.3464],\n",
      "         [-1144.1592,  3551.8147,  4128.0234,  -560.4888,  6200.8604,\n",
      "           5653.5254, -3068.6418,   127.0918, -3332.1958,   159.4605,\n",
      "          -3605.9902, -3332.1958,  1163.8994,  6200.8604,  2995.2048,\n",
      "           3551.8140,  3551.8147,  3551.8147,  2433.4053,  2433.4053,\n",
      "           2433.4053,  2433.4053,  2433.4050,  2433.4050,  2433.4053,\n",
      "           2433.4053,  2433.4053,  2433.4053,  2433.4053,  2433.4053,\n",
      "           2433.4055,  2433.4055],\n",
      "         [-1243.6890,  3398.2759,  3559.7253, -1603.0244,  5001.8857,\n",
      "           3187.9189, -4006.1833, -1715.3417, -2968.2458,   553.0869,\n",
      "           -672.8952, -2968.2458,  -408.7527,  5001.8857,  2167.2124,\n",
      "           3398.2747,  3398.2759,  3398.2759,  -360.3900,  -360.3900,\n",
      "           -360.3900,  -360.3900,  -360.3900,  -360.3900,  -360.3900,\n",
      "           -360.3900,  -360.3900,  -360.3900,  -360.3900,  -360.3900,\n",
      "           -360.3898,  -360.3898],\n",
      "         [ 1734.4742, -1441.3900,   278.1207,   822.3282,  2383.6411,\n",
      "           1560.8959, -2880.6689,  -148.3512, -6694.0601, -3945.5991,\n",
      "          -2172.2930, -6694.0601,  1989.2495,  2383.6411,  2393.5620,\n",
      "          -1441.3906, -1441.3900, -1441.3900, -1109.3389, -1109.3389,\n",
      "          -1109.3389, -1109.3389, -1109.3398, -1109.3398, -1109.3389,\n",
      "          -1109.3389, -1109.3389, -1109.3389, -1109.3389, -1109.3389,\n",
      "          -1109.3392, -1109.3392],\n",
      "         [-2181.9460,   464.4581,  -766.8737,  -815.3420, -2184.1396,\n",
      "           2036.1665,  3743.1345,  6148.6279,  6136.1289, -2548.2510,\n",
      "          -1914.9270,  6136.1289, -3083.3865, -2184.1396, -1613.4376,\n",
      "            464.4585,   464.4581,   464.4581,  3691.5293,  3691.5293,\n",
      "           3691.5293,  3691.5293,  3691.5298,  3691.5298,  3691.5293,\n",
      "           3691.5293,  3691.5293,  3691.5293,  3691.5293,  3691.5293,\n",
      "           3691.5303,  3691.5303],\n",
      "         [-3222.8022,  2466.1958,  1271.2273,  -269.4950, -1811.2623,\n",
      "           3033.6672,  3049.3977,  6344.0586,  5981.8262,  -679.7992,\n",
      "          -1546.1648,  5981.8262, -3886.4312, -1811.2623,   137.6751,\n",
      "           2466.1980,  2466.1958,  2466.1958,  3129.7092,  3129.7092,\n",
      "           3129.7092,  3129.7092,  3129.7100,  3129.7100,  3129.7092,\n",
      "           3129.7092,  3129.7092,  3129.7092,  3129.7092,  3129.7092,\n",
      "           3129.7100,  3129.7100],\n",
      "         [  -85.5203,  2038.7393,  2842.2693,   160.1129,  4813.5200,\n",
      "           4648.9482,  -477.7332,  3982.2854, -2136.7910, -2824.6177,\n",
      "          -1734.4135, -2136.7910, -1375.9042,  4813.5200,   231.3314,\n",
      "           2038.7393,  2038.7393,  2038.7393,  3990.1526,  3990.1526,\n",
      "           3990.1526,  3990.1526,  3990.1521,  3990.1521,  3990.1526,\n",
      "           3990.1526,  3990.1526,  3990.1526,  3990.1526,  3990.1526,\n",
      "           3990.1526,  3990.1526],\n",
      "         [-3614.5916,  3656.8379,  1910.3705,  -604.0681, -1878.5574,\n",
      "           1575.4167,  2494.8770,  3731.3931,  2837.9846, -1812.9316,\n",
      "          -1210.6399,  2837.9846, -3987.0669, -1878.5574,  1429.4781,\n",
      "           3656.8386,  3656.8379,  3656.8379,  3387.1863,  3387.1863,\n",
      "           3387.1863,  3387.1863,  3387.1875,  3387.1875,  3387.1863,\n",
      "           3387.1863,  3387.1863,  3387.1863,  3387.1863,  3387.1863,\n",
      "           3387.1873,  3387.1873],\n",
      "         [ -459.2625,  5359.0239,  3919.2480, -4785.5068,  2005.1471,\n",
      "           3081.2139, -4085.3560,   284.0468, -5661.7710,   -26.8102,\n",
      "             94.4068, -5661.7710, -5475.3770,  2005.1471,   822.8931,\n",
      "           5359.0220,  5359.0239,  5359.0239, -2021.3677, -2021.3677,\n",
      "          -2021.3677, -2021.3677, -2021.3674, -2021.3674, -2021.3677,\n",
      "          -2021.3677, -2021.3677, -2021.3677, -2021.3677, -2021.3677,\n",
      "          -2021.3674, -2021.3674],\n",
      "         [ -893.1982, -2075.2244, -3070.6699,  -591.4589, -5106.2388,\n",
      "           2142.6299,  1249.3546,  3349.2805,   708.5811, -4091.3186,\n",
      "          -2164.3042,   708.5811,  -384.3701, -5106.2388, -2228.6592,\n",
      "          -2075.2239, -2075.2244, -2075.2244, -1183.6984, -1183.6984,\n",
      "          -1183.6984, -1183.6984, -1183.6995, -1183.6995, -1183.6984,\n",
      "          -1183.6984, -1183.6984, -1183.6984, -1183.6984, -1183.6984,\n",
      "          -1183.6993, -1183.6993]]], grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 11, 32])\n",
      "torch.Size([2, 11, 32])\n",
      "torch.Size([2, 11, 64])\n",
      "torch.Size([11, 128])\n",
      "torch.Size([11, 128])\n",
      "torch.Size([11, 128])\n",
      "torch.Size([11, 128])\n",
      "torch.Size([11, 128])\n",
      "LOGITS:  torch.Size([11, 13])\n",
      "torch.Size([11, 13])\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer()\n",
    "probabilities = transformer(ip_tokens = sample_ip_tokens, op_tokens = sample_op_tokens, num_padded_tokens_in_ip = padded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4c52806c-0265-4a71-800e-9fac6552d804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "temp = [1,2,3]\n",
    "def func(x):\n",
    "    x.append(4)\n",
    "func(temp)\n",
    "print(temp)\n",
    "#If you ran decoder earlier (decoder = Decoder()) snippet earlier, then those tokens were modified with padding to matcht the max_seq_len\n",
    "#Secondly, when you had a sequence length of exactly equal to MAX_SEQ_LEN (then, there was a small bug while filling the mask : mas[-num_padded_token:0, :])\n",
    "#so, if you had 0 padded tokens, this snippet was initializing all of the rows to inf in mask, I fixed that, so the first bug, help me identify second bug and in end both were resolved\n",
    "\n",
    "#NOW, we need to implement a linear layer that maps 128 sized embedding to a VOCAB_SIZE_LEN_DEC space and then apply softmax over that final step to obtain probabilities over the vocab\n",
    "# We need to do this for all the 11 output embeddings obtained from the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c062f82c-8d55-4a68-9f99-4d52265a10ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 10, 5, 7, 9, 10, 2, 4, 12]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_indices = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "# Convert the result to a Python list if needed\n",
    "max_indices_list = max_indices.tolist()\n",
    "max_indices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c063779b-5a77-40d7-ab08-1dc2e055cac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '2', '4', '9', '4', '6', '8', '9', '1', '3', '<sos']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_tokens = [itoc_dec[idx] for idx in max_indices_list]\n",
    "decoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c79b7be2-ec4d-4f85-b5f1-39e8d2413b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0249468913<sos\n"
     ]
    }
   ],
   "source": [
    "print(''.join(decoded_tokens))\n",
    "#Expected output: 2000-11-10<eos>\n",
    "#Hopefully, after proper training with training data, this will improve :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd8f8e5-82d3-4ffc-acd1-9f2ded3bc4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
